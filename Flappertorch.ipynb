{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorbuenog/Flapper/blob/test/Flappertorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "import numpy\n",
        "import scipy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.path as mpath\n",
        "from matplotlib.markers import MarkerStyle\n",
        "import matplotlib.font_manager as fm\n",
        "plt.ioff()\n",
        "\n",
        "import gym\n",
        "from gym import wrappers\n",
        "from gym import spaces\n",
        "\n",
        "import imageio"
      ],
      "metadata": {
        "id": "V1tlyuUFgqtI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QuAwnp9LVMnk"
      },
      "outputs": [],
      "source": [
        "class InitialCondition(object):\n",
        "\n",
        "    def __init__(self, distance=None, f2=None, A2=None, goal=None):\n",
        "        self.distance = distance if distance is not None else 21.5\n",
        "        self.A1 = 2.0\n",
        "        self.f1 = 1.0\n",
        "        self.A2 = 2.0 if A2 is None else A2\n",
        "        self.f2 = 1.0 if f2 is None else f2\n",
        "        self.goal = goal if goal is not None else 21.5\n",
        "\n",
        "        self.u2 = numpy.pi * self.A2 * self.f2 * numpy.sqrt(2 * SwimmerEnv.Ct / SwimmerEnv.Cd)\n",
        "        self.v2 = -self.A2*(2 * numpy.pi * self.f2)\n",
        "        self.t_delay = -self.distance/self.u2\n",
        "        self.v_flow = self.A1*self.f1*numpy.cos(2*numpy.pi*self.f1*(-self.t_delay))*numpy.exp(-self.t_delay/SwimmerEnv.T)\n",
        "        self.flow_agreement = self.v2 * self.v_flow\n",
        "\n",
        "    def random(self, randomize_fields=[]):\n",
        "        if 'distance' in randomize_fields:\n",
        "            self.distance = random.uniform(10,30)\n",
        "        if 'f2' in randomize_fields:\n",
        "            self.f2 = random.uniform(0.5, 1.5)\n",
        "        if 'A2' in randomize_fields:\n",
        "            self.A2 = random.uniform(.5, 3.0)\n",
        "        if 'v2' in randomize_fields:\n",
        "            self.v2 = random.uniform(-1.0, 1.0)\n",
        "        return self\n",
        "\n",
        "\n",
        "class SwimmerEnv(gym.Env):\n",
        "\n",
        "    s = 15.\n",
        "    c = 4.\n",
        "    As = s * c\n",
        "    T = 5\n",
        "    m = 80.\n",
        "    Ct = .96\n",
        "    Cd = .25\n",
        "    rho = 1.\n",
        "    dt = 0.1\n",
        "\n",
        "    def __init__(self, rewards=[]):\n",
        "        super(SwimmerEnv, self).__init__()\n",
        "\n",
        "        self.action_space = spaces.Discrete(5) # Increase, decrease, or maintain value for f2 and A2\n",
        "        self.n_history = 50 # observation timesteps\n",
        "\n",
        "        # Observations: f2, A2, distance, v2-v_flow, u2\n",
        "        obs_low = [-numpy.inf for _ in range(5*self.n_history)]\n",
        "        obs_high = [numpy.inf for _ in range(5*self.n_history)]\n",
        "\n",
        "        self.observation_space = spaces.Box(low=numpy.array(obs_low), high=numpy.array(obs_high), dtype=numpy.float32)\n",
        "        self.rewards = rewards\n",
        "\n",
        "        self.flap = None\n",
        "        self.t_bound = 500.\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def _shoot(self, A1, A2, f1, f2, vec_initial, t_start=0., t_bound=5000., method='RK45'):\n",
        "        rho = self.rho\n",
        "        As = self.As\n",
        "        T = self.T\n",
        "        m = self.m\n",
        "        Ct = self.Ct\n",
        "        Cd = self.Cd\n",
        "        s = self.s\n",
        "        c = self.c\n",
        "\n",
        "        def fun(t, vec):\n",
        "            x2, u2 = vec\n",
        "            u1 = numpy.pi * A1 * f1 * numpy.sqrt(2 * Ct / Cd)  # Leader velocity (constant)\n",
        "            dt = -x2 / u1 + t\n",
        "            self.Ft2 = 2*rho*As*Ct*numpy.pi**2*((A2*f2*numpy.cos(2*numpy.pi*f2*t)-A1*f1*numpy.cos(2*numpy.pi*f1*(t-dt))*numpy.exp(-dt/T)))**2\n",
        "            self.Fd2 = rho*As*Cd*u2**2/2\n",
        "            dy_dt = (u2, (self.Ft2 - self.Fd2)/m)\n",
        "            return numpy.asarray(dy_dt)\n",
        "        # events = [lambda t, y: y[0] - y[4] - 0.00001]\n",
        "        events = []\n",
        "        for ee in events: setattr(ee, 'terminal', True)\n",
        "        solver = scipy.integrate.solve_ivp(fun, (t_start, t_bound), vec_initial, method=method, events=events,\n",
        "                                            rtol=1e-4, atol=1e-7, max_step=.03, first_step=.001, dense_output=True)\n",
        "        u1 = numpy.pi * A1 * f1 * numpy.sqrt(2 * Ct / Cd)  # Leader velocity (constant)\n",
        "        x1 = u1 * solver.t[-1]  # Leader x-coord (calculated)\n",
        "        y1 = A1 * numpy.sin(2 * numpy.pi * f1 * solver.t[-1])  # Leader y-coord (calculated)\n",
        "        v1 = -A1*(2 * numpy.pi * f1)*numpy.cos(2 * numpy.pi * f1 * solver.t[-1])\n",
        "        y2 = A2 * numpy.sin(2 * numpy.pi * f2 * solver.t[-1])  # Leader y-coord (calculated)\n",
        "        v2 = -A2*(2 * numpy.pi * f2)*numpy.cos(2 * numpy.pi * f2 * solver.t[-1])\n",
        "        values = list(zip(solver.t, solver.y.T))\n",
        "\n",
        "        info = {\n",
        "            'x1': x1,\n",
        "            'y1': y1,\n",
        "            'u1': u1,\n",
        "            'v1': v1,\n",
        "            'x2': values[-1][1][0],\n",
        "            'y2': y2,\n",
        "            'u2': values[-1][1][1],\n",
        "            'v2': v2,\n",
        "        }\n",
        "        return solver, values, info\n",
        "\n",
        "\n",
        "    def _get_obs(self):\n",
        "        obs = [\n",
        "            self.f2,\n",
        "            self.A2,\n",
        "            self.distance,\n",
        "            self.v2 - self.v_flow,\n",
        "            self.avg_u2,\n",
        "        ]\n",
        "\n",
        "        return numpy.array(obs, dtype=numpy.float32)\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        Ct = self.Ct\n",
        "        Cd = self.Cd\n",
        "\n",
        "        match action:\n",
        "            case 0:\n",
        "                self.f2 = max(self.f2 - 0.1, 0.5)\n",
        "            case 1:\n",
        "                self.f2 = min(self.f2 + 0.1, 1.5)\n",
        "            case 2:\n",
        "                self.A2 = max(self.A2 - 0.1, 0.5)\n",
        "            case 3:\n",
        "                self.A2 = min(self.A2 + 0.1, 3.0)\n",
        "            case 4: # No change\n",
        "                self.A2 = self.A2\n",
        "                self.f2 = self.f2\n",
        "\n",
        "        t_bound_step = self.dt\n",
        "        solver, values, shoot_info = self._shoot(self.A1, self.A2, self.f1, self.f2, self.flap, t_start=self.tt, t_bound=self.tt+t_bound_step)\n",
        "        self.flap = values[-1][1]\n",
        "        self.tt += self.dt\n",
        "        self.u1 = numpy.pi * self.A1 * self.f1 * numpy.sqrt(2 * Ct / Cd)  # Leader velocity\n",
        "        self.x1 = self.u1 * self.tt  # Leader x-coord\n",
        "        self.y1 = self.A1 * numpy.sin(2 * numpy.pi * self.f1 * self.tt)  # Leader y-coord\n",
        "        self.y2 = self.A2 * numpy.sin(2 * numpy.pi * self.f2 * self.tt)  # Follower y-coord\n",
        "        self.v1 = -self.A1*(2 * numpy.pi * self.f1)*numpy.cos(2 * numpy.pi * self.f1 * self.tt)\n",
        "        self.t_delay = self.tt - self.flap[0]/self.u1\n",
        "        v_flow = self.A1*self.f1*numpy.cos(2*numpy.pi*self.f1*(self.tt-self.t_delay))*numpy.exp(-self.t_delay/self.T)\n",
        "        self.t_delay_head = self.tt - (self.flap[0]+self.c)/self.u1\n",
        "        v_flow_head = self.A1*self.f1*numpy.cos(2*numpy.pi*self.f1*(self.tt-self.t_delay_head))*numpy.exp(-self.t_delay_head/self.T)\n",
        "        v_gradient = (v_flow_head - v_flow)/self.c\n",
        "        self.u2 = self.flap[1]\n",
        "\n",
        "        alpha = -numpy.arctan2(shoot_info['v2']-v_flow, self.u2)\n",
        "        Cl = 2*numpy.pi*numpy.sin(alpha)\n",
        "        v2_dot = (shoot_info['v2'] - self.previous_v2)/self.dt\n",
        "        # L = Cl*self.rho*self.As*self.u2**2/2\n",
        "        # Fy = self.m*v2_dot - L*numpy.cos(-alpha)\n",
        "        L = 1*self.rho*self.As*(shoot_info['v2']-v_flow)**2/2\n",
        "        Fy = self.m*v2_dot - L\n",
        "        self.power = (Fy*shoot_info['v2']) / 1e5\n",
        "\n",
        "        self.distance = self.x1 - self.flap[0]-self.c  # Distance between leader and follower\n",
        "        done = False\n",
        "        new_distance_from_goal = numpy.abs(self.distance - self.goal) # Distance\n",
        "        new_flow_agreement = shoot_info['v2'] * v_flow_head # Flow agreement\n",
        "        a_flow = (v_flow-self.previous_v_flow)/self.dt # Flow acceleration\n",
        "\n",
        "\n",
        "        # Calculate average flow agreement over one period\n",
        "        self.flow_agreement_history.append(new_flow_agreement)\n",
        "        if len(self.flow_agreement_history) > 10:\n",
        "            self.flow_agreement_history.pop(0)\n",
        "        new_avg_flow_agreement = numpy.mean(self.flow_agreement_history)\n",
        "\n",
        "        # Calculate average follower velocity over one period\n",
        "        self.u2_history.append(self.u2)\n",
        "        if len(self.u2_history) > 10:\n",
        "            self.u2_history.pop(0)\n",
        "        self.avg_u2 = numpy.mean(self.u2_history)\n",
        "\n",
        "        # Power average\n",
        "        self.power_history.append(self.power)\n",
        "        if len(self.power_history) > 10:\n",
        "            self.power_history.pop(0)\n",
        "        self.avg_power = numpy.mean(self.power_history)\n",
        "\n",
        "        # Reset reward\n",
        "        self.reward = 0.\n",
        "\n",
        "        # Penalize change in distance\n",
        "        self.reward -= 0.2*abs(self.distance - self.previous_distance)\n",
        "\n",
        "        # Penalize extreme behaviors\n",
        "        if self.distance < 0. or self.distance > 200:\n",
        "            self.reward -= 100\n",
        "            done = True\n",
        "\n",
        "        self.previous_power = self.avg_power\n",
        "        self.previous_v_flow = v_flow\n",
        "        self.distance_from_goal = new_distance_from_goal\n",
        "        self.flow_agreement = new_flow_agreement\n",
        "        self.avg_flow_agreement = new_avg_flow_agreement\n",
        "        self.last_reward = self.reward\n",
        "        self.previous_v2 = shoot_info['v2']\n",
        "        self.previous_distance = self.distance\n",
        "\n",
        "        # Update observation history\n",
        "        self.obs_history.append(self._get_obs())\n",
        "        if len(self.obs_history) > self.n_history:\n",
        "            self.obs_history.pop(0)\n",
        "        history_obs = numpy.array(self.obs_history).flatten()\n",
        "\n",
        "        info = {\n",
        "            'distance': self.distance,\n",
        "            'action': action,\n",
        "            'reward': self.reward,\n",
        "            'done': done,\n",
        "            'f1': self.f1,\n",
        "            'f2': self.f2,\n",
        "            'A2': self.A2,\n",
        "            'distance_from_goal': self.distance_from_goal,\n",
        "            't': self.tt,\n",
        "            'v_flow': v_flow,\n",
        "            'v_flow_head': v_flow_head,\n",
        "            'a_flow': a_flow,\n",
        "            'flow_agreement': new_flow_agreement,\n",
        "            'avg_flow_agreement': new_avg_flow_agreement,\n",
        "            'u2': self.flap[1],\n",
        "            'avg_u2': self.avg_u2,\n",
        "            'v_gradient': v_gradient,\n",
        "            'avg_power': self.avg_power,\n",
        "        }\n",
        "        info.update(shoot_info)\n",
        "\n",
        "        return history_obs, self.reward, done, info\n",
        "\n",
        "    def reset(self, initial_condition=None, initial_condition_fn=None):\n",
        "        Ct = self.Ct\n",
        "        Cd = self.Cd\n",
        "        gap_distance_in_wavelengths = 1.\n",
        "\n",
        "        # get initial condition\n",
        "        if initial_condition is None:\n",
        "            if initial_condition_fn is not None:\n",
        "                initial_condition = initial_condition_fn()\n",
        "            else:\n",
        "                initial_condition = InitialCondition()\n",
        "\n",
        "        self.A1 = initial_condition.A1\n",
        "        self.f1 = initial_condition.f1\n",
        "        self.A2 = initial_condition.A2\n",
        "        self.f2 = initial_condition.f2\n",
        "        self.goal = initial_condition.goal\n",
        "\n",
        "        u2_initial = initial_condition.u2\n",
        "\n",
        "        self.distance = initial_condition.distance\n",
        "        self.distance_from_goal = numpy.abs(self.distance - self.goal)\n",
        "        self.flow_agreement = initial_condition.flow_agreement\n",
        "        self.flap = numpy.asarray([-initial_condition.distance, u2_initial])\n",
        "        self.tt = 0\n",
        "        self.previous_v_flow = 0\n",
        "        self.flow_agreement_history = []\n",
        "        self.u2_history = []\n",
        "        self.power_history = []\n",
        "        self.previous_x2 = -self.distance\n",
        "        self.previous_distance = self.distance\n",
        "        self.flow_agreement = initial_condition.flow_agreement\n",
        "        self.avg_flow_agreement = 0\n",
        "        self.u2 = .1\n",
        "        self.avg_u2 = .1\n",
        "        self.avg_power = .1\n",
        "        self.previous_power = .1\n",
        "        self.previous_v2 = .1\n",
        "        self.v2 = initial_condition.v2\n",
        "        self.v_flow = 0\n",
        "\n",
        "        self.obs_history = [self._get_obs() for _ in range(self.n_history)]\n",
        "\n",
        "        history_obs = numpy.array(self.obs_history).flatten()\n",
        "\n",
        "        return history_obs\n",
        "\n",
        "    def render(self, mode='rgb_array'):\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.scatter(self.x1, self.y1, label='leader')\n",
        "        ax.scatter([self.flap[0]], [self.flap[1]], label='follower')\n",
        "        ax.set_xlim([self.x1 - 50, self.x1 + 5])\n",
        "        ax.set_ylim([-2*self.A1, 2*self.A1])\n",
        "        ax.legend()\n",
        "        fig.canvas.draw()\n",
        "        fig.canvas.tostring_rgb()\n",
        "        frame = numpy.frombuffer(fig.canvas.tostring_rgb(), dtype=numpy.uint8)\n",
        "        frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "        plt.close(fig)\n",
        "        return frame\n",
        "\n",
        "    def close(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "Kp-2bJ7mSifa"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Memory(object):\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "    def clear_memory(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.is_terminals[:]\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        # actor\n",
        "        self.action_layer = nn.Sequential(\n",
        "                nn.Linear(state_dim, n_latent_var),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(n_latent_var, n_latent_var),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(n_latent_var, action_dim),\n",
        "                nn.Softmax(dim=-1)\n",
        "                )\n",
        "\n",
        "        # critic\n",
        "        self.value_layer = nn.Sequential(\n",
        "                nn.Linear(state_dim, n_latent_var),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(n_latent_var, n_latent_var),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(n_latent_var, 1)\n",
        "                )\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def act(self, state, memory):\n",
        "        state = torch.from_numpy(state).float().to(device)\n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "\n",
        "        memory.states.append(state)\n",
        "        memory.actions.append(action)\n",
        "        memory.logprobs.append(dist.log_prob(action))\n",
        "\n",
        "        return action.item()\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "\n",
        "        state_value = self.value_layer(state)\n",
        "\n",
        "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
        "\n",
        "class PPO(object):\n",
        "    def __init__(self, state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip):\n",
        "        self.lr = lr\n",
        "        self.betas = betas\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "    def update(self, memory):\n",
        "        # Monte Carlo estimate of state rewards:\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        # Normalizing the rewards:\n",
        "        rewards = torch.tensor(rewards).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
        "        rewards = rewards.type(torch.float32)\n",
        "\n",
        "        # convert list to tensor\n",
        "        old_states = torch.stack(memory.states).to(device).detach()\n",
        "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
        "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
        "\n",
        "        # Optimize policy for K epochs:\n",
        "        for _ in range(self.K_epochs):\n",
        "            # Evaluating old actions and values :\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # Ensure state_values are float32\n",
        "            state_values = state_values.type(torch.float32)\n",
        "\n",
        "            # Finding the ratio (pi_theta / pi_theta__old):\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Finding Surrogate Loss:\n",
        "            advantages = rewards - state_values.detach()\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
        "\n",
        "            # take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Copy new weights into old policy:\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, **env_args):\n",
        "        ############## Hyperparameters ##############\n",
        "        self.env_name = \"Flappers\"\n",
        "        self.env = SwimmerEnv(**env_args)\n",
        "        self.state_dim = self.env.observation_space.shape[0]\n",
        "        self.action_dim = 5\n",
        "        self.render = False\n",
        "        self.solved_reward = 500         # stop training if avg_reward > solved_reward\n",
        "        self.log_interval = 20           # print avg reward in the interval\n",
        "        self.max_episodes = 1000        # max training episodes\n",
        "        self.max_timesteps = 500       # max timesteps in one episode\n",
        "        self.n_latent_var = 64           # number of variables in hidden layer\n",
        "        self.update_timestep = 200    # update policy every n timesteps\n",
        "        self.lr = 0.002\n",
        "        self.betas = (0.9, 0.999)\n",
        "        self.gamma = 0.99                # discount factor\n",
        "        self.K_epochs = 4               # update policy for K epochs\n",
        "        self.eps_clip = 0.2              # clip parameter for PPO\n",
        "        self.random_seed = 4\n",
        "        #############################################\n",
        "\n",
        "        if self.random_seed:\n",
        "            torch.manual_seed(self.random_seed)\n",
        "            self.env.seed(self.random_seed)\n",
        "\n",
        "        self.memory = Memory()\n",
        "        self.ppo = PPO(self.state_dim, self.action_dim, self.n_latent_var, self.lr, self.betas, self.gamma, self.K_epochs, self.eps_clip)\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        self.ppo.policy.load_state_dict(torch.load(model_path))\n",
        "        self.ppo.policy_old.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    def get_policy(self, episodes=1000):\n",
        "        pol_name = f\"PPO_{self.env_name}_{episodes}.pth\"\n",
        "        return pol_name\n",
        "\n",
        "    def train(self, initial_condition=None, initial_condition_fn=None, episodes=None):\n",
        "\n",
        "        # logging variables\n",
        "        running_reward = 0\n",
        "        avg_length = 0\n",
        "        timestep = 0\n",
        "        history = []\n",
        "\n",
        "        # header\n",
        "        with open(f\"log_PPO_{self.env_name}.csv\", 'a') as f:\n",
        "                    f.write(f'Episode,avg length,reward\\n')\n",
        "        # training loop\n",
        "        if episodes is None:\n",
        "            episodes = self.max_episodes\n",
        "        for i_episode in range(1, episodes+1):\n",
        "            state = self.env.reset(initial_condition=initial_condition, initial_condition_fn=initial_condition_fn)\n",
        "            for t in range(self.max_timesteps):\n",
        "                timestep += 1\n",
        "\n",
        "                # Running policy_old:\n",
        "                action = self.ppo.policy_old.act(state, self.memory)\n",
        "                state, reward, done, info = self.env.step(action)\n",
        "\n",
        "                # Saving reward and is_terminal:\n",
        "                self.memory.rewards.append(reward)\n",
        "                self.memory.is_terminals.append(done)\n",
        "\n",
        "                # update if its time\n",
        "                if timestep % self.update_timestep == 0:\n",
        "                    self.ppo.update(self.memory)\n",
        "                    self.memory.clear_memory()\n",
        "                    timestep = 0\n",
        "\n",
        "                running_reward += reward\n",
        "                if self.render:\n",
        "                    self.env.render()\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "                history.append(info)\n",
        "\n",
        "            avg_length += t\n",
        "\n",
        "            filename = f\"./PPO_{self.env_name}_{i_episode}.pth\"\n",
        "\n",
        "            # save every 1000 episodes\n",
        "            if i_episode % 1000 == 0:\n",
        "                    torch.save(self.ppo.policy.state_dict(), filename)\n",
        "            # logging\n",
        "            if i_episode % self.log_interval == 0:\n",
        "                avg_length = int(avg_length/self.log_interval)\n",
        "                running_reward = ((running_reward/self.log_interval))\n",
        "\n",
        "                print('Episode {} \\t avg length: {} \\t reward: {}'.format(i_episode, avg_length, running_reward))\n",
        "\n",
        "                # Save message on csv file\n",
        "                with open(f\"log_PPO_{self.env_name}.csv\", 'a') as f:\n",
        "                    f.write(f'{i_episode},{avg_length},{running_reward}\\n')\n",
        "                running_reward = 0\n",
        "                avg_length = 0\n",
        "\n",
        "        return history;\n",
        "\n",
        "    def plot_policy(self, fa_range=None, f2_range=None, a2_range=None, d_range=None, u_range=None, p_range=None):\n",
        "            action_choices = []\n",
        "\n",
        "            ranges = {\n",
        "                'flow agreement': fa_range,\n",
        "                'avg flow agreement': fa_range,\n",
        "                'distance': d_range,\n",
        "                'f2': f2_range,\n",
        "                'A2': a2_range,\n",
        "                'velocity': u_range,\n",
        "                'power': p_range\n",
        "            }\n",
        "\n",
        "            # Reorder to match obs list\n",
        "            obs_order = [obs_name for obs_name in ['distance', 'f2', 'A2', 'flow agreement', 'avg flow agreement', 'velocity'] if obs_name in self.env.observations]\n",
        "\n",
        "            range1 = ranges[obs_order[0]]\n",
        "            range2 = ranges[obs_order[1]]\n",
        "\n",
        "            for x1 in range1:\n",
        "                for x2 in range2:\n",
        "                    state = numpy.array([x1, x2], dtype=numpy.float32)\n",
        "                    state_tensor = torch.from_numpy(state).float().to(device)\n",
        "                    with torch.no_grad():\n",
        "                        action_probs = self.ppo.policy.action_layer(state_tensor).cpu().numpy()\n",
        "                    action = numpy.argmax(action_probs)\n",
        "                    # print(f\"Obs: {x1} {x2} Action: {action}\")\n",
        "                    action_choices.append((x1, x2, action))\n",
        "\n",
        "            action_choices = numpy.array(action_choices)\n",
        "            # print(action_choices.shape)\n",
        "\n",
        "            plt.figure(figsize=(12, 6))\n",
        "\n",
        "            scatter = plt.scatter(action_choices[:, 0], action_choices[:, 1], c=action_choices[:, 2], cmap='viridis', marker='o')\n",
        "            handles = [plt.Line2D([], [], color=scatter.cmap(scatter.norm(i)), marker='o', linestyle='', markersize=5, label=label)\n",
        "                      for i, label in enumerate(['Reduce', 'Maintain', 'Increase'])]\n",
        "            plt.legend(handles=handles, title='Chosen Action')\n",
        "            plt.title(f'Chosen Action Based on State ({obs_order[0]} and {obs_order[1]})')\n",
        "            plt.xlabel(obs_order[0])\n",
        "            plt.ylabel(obs_order[1])\n",
        "            plt.xticks(numpy.linspace(min(range1), max(range1), 11))\n",
        "            plt.yticks(numpy.linspace(min(range2), max(range2), 11))\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "    def test_initial_conditions(self, initial_conditions):\n",
        "\n",
        "        max_timesteps = 500\n",
        "\n",
        "        fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1, figsize=(10, 20))\n",
        "\n",
        "        for ic in initial_conditions:\n",
        "            obs = self.env.reset(initial_condition=ic)\n",
        "            schooling_numbers = []\n",
        "            flow_agreements = []\n",
        "            avg_flow_agreements = []\n",
        "            v_gradients = []\n",
        "            u2 = []\n",
        "            action_values = []\n",
        "            p_values = []\n",
        "            for t in range(max_timesteps):\n",
        "                action = self.ppo.policy.act(obs, Memory())\n",
        "                obs, reward, done, info = self.env.step(action)\n",
        "                schooling_number = (info['distance']-SwimmerEnv.c)/info['u1']/info['f1']\n",
        "                schooling_numbers.append(schooling_number)\n",
        "                flow_agreements.append(info['flow_agreement'])\n",
        "                avg_flow_agreements.append(info['avg_flow_agreement'])\n",
        "                v_gradients.append(info['v_gradient'])\n",
        "                u2.append(info['avg_u2'])\n",
        "                p_values.append(info['avg_power'])\n",
        "                if self.env.action is not None:\n",
        "                    key = self.env.action\n",
        "                else:\n",
        "                    key = self.env.observations[0]\n",
        "                action_values.append(info[key])\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            # plt.plot(distances, label=f\"Initial Distance: {ic.distance}\")\n",
        "            ax1.plot(schooling_numbers, label=f\"Initial Distance: {ic.distance}\")\n",
        "\n",
        "            ax2.plot(p_values, label=f\"Initial Distance: {ic.distance}\")\n",
        "\n",
        "            ax3.plot(avg_flow_agreements, label=f\"Initial Distance: {ic.distance}\")\n",
        "\n",
        "            ax4.plot(v_gradients, label=f\"Initial Distance: {ic.distance}\")\n",
        "\n",
        "            ax5.plot(u2, label=f\"Initial Distance: {ic.distance}\")\n",
        "\n",
        "        ax1.set_xlabel('Timesteps (0.1 s)')\n",
        "        ax1.set_ylabel('Schooling Number')\n",
        "        ax1.set_title('Distance Between Leader and Follower for Different Initial Conditions')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        ax2.set_xlabel('Timesteps (0.1 s)')\n",
        "        ax2.set_ylabel('Average Power')\n",
        "        # ax2.set_xlim([0, 100])\n",
        "        ax2.set_title('Power for Different Initial Conditions')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        ax3.set_xlabel('Timesteps (0.1 s)')\n",
        "        ax3.set_ylabel('Average Flow Agreement')\n",
        "        # ax3.set_xlim([0, 100])\n",
        "        ax3.set_title('Average Flow Agreement for Different Initial Conditions')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True)\n",
        "\n",
        "        ax4.set_xlabel('Timesteps (0.1 s)')\n",
        "        ax4.set_ylabel('Velocity Gradient')\n",
        "        # ax4.set_xlim([0,100])\n",
        "        std_vel_gradient = numpy.std(v_gradients)\n",
        "        ax4.set_ylim([-3*std_vel_gradient,3*std_vel_gradient])\n",
        "        ax4.set_title('Velocity Gradient for Different Initial Conditions')\n",
        "        ax4.legend()\n",
        "        ax4.grid(True)\n",
        "\n",
        "        ax5.set_xlabel('Timesteps (0.1 s)')\n",
        "        ax5.set_ylabel('Horizontal Velocity')\n",
        "        # ax5.set_xlim([0,100])\n",
        "        ax5.set_title('Average Velocity for Different Initial Conditions')\n",
        "        ax5.legend()\n",
        "        ax5.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def naca_airfoil(self, code, num_points=100):\n",
        "        \"\"\"Generates the coordinates of a NACA 4-digit airfoil.\"\"\"\n",
        "        m = float(code[0]) / 100.0  # Maximum camber\n",
        "        p = float(code[1]) / 10.0  # Location of maximum camber\n",
        "        t = float(code[2:]) / 100.0  # Maximum thickness\n",
        "\n",
        "        x = numpy.linspace(0, 1, num_points)\n",
        "        yt = 5 * t * (0.2969 * numpy.sqrt(x) - 0.1260 * x - 0.3516 * x**2 + 0.2843 * x**3 - 0.1015 * x**4)\n",
        "\n",
        "        if m == 0 and p == 0:\n",
        "            yc = numpy.zeros_like(x)\n",
        "            theta = numpy.zeros_like(x)\n",
        "        else:\n",
        "            yc = numpy.where(x <= p,\n",
        "                            m / p**2 * (2 * p * x - x**2),\n",
        "                            m / (1 - p)**2 * ((1 - 2 * p) + 2 * p * x - x**2))\n",
        "            theta = numpy.arctan(numpy.gradient(yc, x))\n",
        "\n",
        "        xu = x - yt * numpy.sin(theta)\n",
        "        yu = yc + yt * numpy.cos(theta)\n",
        "        xl = x + yt * numpy.sin(theta)\n",
        "        yl = yc - yt * numpy.cos(theta)\n",
        "\n",
        "        # Close the path by adding the first point to the end\n",
        "        x_coords = -numpy.concatenate([xu, xl[::-1]])\n",
        "        y_coords = numpy.concatenate([yu, yl[::-1]])\n",
        "\n",
        "        return mpath.Path(numpy.column_stack([x_coords, y_coords]))\n",
        "\n",
        "    def animate(self, i):\n",
        "        font_prop = fm.FontProperties(size=35)\n",
        "        self.ax.clear()\n",
        "        x_leader = self.leader_positions[i][0]\n",
        "        y_leader = self.leader_positions[i][1]\n",
        "        x_follower = self.follower_positions[i][0]\n",
        "        y_follower = self.follower_positions[i][1]\n",
        "        self.ax.set_xlim([x_leader - 50, x_leader + 5])\n",
        "        self.ax.set_ylim([-1.5 * self.env.A1, 1.5 * self.env.A1])\n",
        "        self.ax.scatter(x_leader, y_leader, label='leader', color='black', marker=self.fish_marker)\n",
        "        self.ax.scatter(x_follower, y_follower, label='follower', color='black', marker=self.fish_marker)\n",
        "        self.ax.grid(True, axis='x', color='grey')\n",
        "        self.ax.set_xlabel('X Position (cm)', fontproperties = font_prop)\n",
        "        self.ax.set_ylabel('Y Position (cm)', fontproperties = font_prop)\n",
        "        self.ax.tick_params(axis='both', which='major', labelsize=30)\n",
        "\n",
        "        # Display wake\n",
        "        T = self.env.T\n",
        "        for j, wake_pos in enumerate(self.leader_positions[:i]):\n",
        "            t_delay = (i - j) / self.frame_rate\n",
        "            wake_amplitude_scale = numpy.exp(-t_delay / T)\n",
        "            self.ax.scatter(wake_pos[0], wake_pos[1]*wake_amplitude_scale, color='blue', s=5, marker='o')\n",
        "\n",
        "        return self.ax\n",
        "\n",
        "    def create_video(self, ic=InitialCondition(distance=30, f2=1.), time=10):\n",
        "        state = self.env.reset(ic)\n",
        "\n",
        "        airfoil_path = self.naca_airfoil(\"0017\")\n",
        "        self.fish_marker = MarkerStyle(airfoil_path, transform=mpl.transforms.Affine2D().scale(16))\n",
        "\n",
        "        self.leader_positions = []\n",
        "        self.follower_positions = []\n",
        "        self.frame_rate = 12\n",
        "        runtime = time # seconds\n",
        "\n",
        "        for _ in range(self.frame_rate*runtime):\n",
        "            action = self.ppo.policy.act(state, Memory())\n",
        "            state, reward, done, info = self.env.step(action)\n",
        "\n",
        "            self.leader_positions.append((info['x1'], info['y1']))\n",
        "            self.follower_positions.append((info['x2'], info['y2']))\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        fig, self.ax = plt.subplots(figsize=(21, 9))\n",
        "        ani = animation.FuncAnimation(fig, self.animate, frames=len(self.leader_positions), interval=self.frame_rate, blit=False)\n",
        "\n",
        "        # To save the animation as a video file:\n",
        "        ani.save('swimmer_animation.mp4', writer='ffmpeg', fps=self.frame_rate)\n",
        "\n",
        "        # Plot distances\n",
        "        distances = [numpy.linalg.norm(numpy.array(leader_pos) - numpy.array(follower_pos))\n",
        "                    for leader_pos, follower_pos in zip(self.leader_positions, self.follower_positions)]\n",
        "        times = numpy.arange(len(distances)) * (1.0 / self.frame_rate)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(times, distances)\n",
        "        plt.xlabel('Time (seconds)')\n",
        "        plt.ylabel('Distance')\n",
        "        plt.title('Distance between Leader and Follower vs. Time')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        return;"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL CONTROL**"
      ],
      "metadata": {
        "id": "4WJ5ECTGRw75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/victorbuenog/Flapper.git"
      ],
      "metadata": {
        "id": "gC5gRtAYMvBD",
        "outputId": "15dc120a-206c-4d59-8451-37a75d5ecb5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Flapper' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full run\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# obs_action = ['f2','A2']\n",
        "# obs_flow = ['distance', 'flow agreement', 'avg flow agreement', 'velocity']\n",
        "# rewards = ['distance', 'flow agreement', 'avg flow agreement', 'velocity', 'flow acceleration']\n",
        "\n",
        "# for obs1 in obs_action:\n",
        "#   for obs2 in obs_flow:\n",
        "#     for rew in rewards:\n",
        "#       trainer = Trainer(\n",
        "#           action=obs1,  # None, 'f2', or 'A2'\n",
        "#           observations=[obs1, obs2],  # may contain: 'distance', 'f2', 'A2', 'flow\n",
        "#           rewards=[rew],  # may contain: 'distance', 'flow agreement'\n",
        "#       )\n",
        "#       icfn = lambda: InitialCondition().random([obs1,'distance'])\n",
        "#       trainer.train(initial_condition_fn=icfn, episodes=1000)\n",
        "\n",
        "#####################################################################################################################\n",
        "\n",
        "# Single Run\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "trainer = Trainer()\n",
        "\n",
        "# train from scratch\n",
        "icfn = lambda: InitialCondition().random(['f2','distance'])\n",
        "history  = trainer.train(initial_condition_fn=icfn, episodes=10000)\n",
        "\n",
        "# load from pth\n",
        "# model_path = f\"Flapper/Policies/{trainer.get_policy(episodes=1000)}\"\n",
        "# print(f\"Loading policy: {model_path}\")\n",
        "# trainer.load_model(model_path)"
      ],
      "metadata": {
        "id": "xbD6nUGvhm4z",
        "outputId": "cb9d4d82-daa9-4b25-a912-703b1daf8ce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20 \t avg length: 199 \t reward: -92.36618475841996\n",
            "Episode 40 \t avg length: 298 \t reward: -97.78783457035141\n",
            "Episode 60 \t avg length: 139 \t reward: -111.53926462943164\n",
            "Episode 80 \t avg length: 121 \t reward: -109.02341944298989\n",
            "Episode 100 \t avg length: 89 \t reward: -105.4832371107731\n",
            "Episode 120 \t avg length: 175 \t reward: -101.3212544911046\n",
            "Episode 140 \t avg length: 240 \t reward: -90.16847437332224\n",
            "Episode 160 \t avg length: 204 \t reward: -111.17021249139327\n",
            "Episode 180 \t avg length: 144 \t reward: -122.20401940775632\n",
            "Episode 200 \t avg length: 202 \t reward: -135.6368371766843\n",
            "Episode 220 \t avg length: 209 \t reward: -136.11517922017688\n",
            "Episode 240 \t avg length: 199 \t reward: -137.30548123802623\n",
            "Episode 260 \t avg length: 197 \t reward: -137.16120438701017\n",
            "Episode 280 \t avg length: 182 \t reward: -136.42251762149994\n",
            "Episode 300 \t avg length: 178 \t reward: -134.53657131645042\n",
            "Episode 320 \t avg length: 171 \t reward: -132.99503677332046\n",
            "Episode 340 \t avg length: 203 \t reward: -139.1054678270255\n",
            "Episode 360 \t avg length: 188 \t reward: -138.65227828146877\n",
            "Episode 380 \t avg length: 210 \t reward: -138.78562659424378\n",
            "Episode 400 \t avg length: 218 \t reward: -137.6217228956785\n",
            "Episode 420 \t avg length: 218 \t reward: -138.47191563608337\n",
            "Episode 440 \t avg length: 221 \t reward: -138.8349292625166\n",
            "Episode 460 \t avg length: 217 \t reward: -138.27829974613206\n",
            "Episode 480 \t avg length: 207 \t reward: -136.68052243141642\n",
            "Episode 500 \t avg length: 212 \t reward: -136.5011958335704\n",
            "Episode 520 \t avg length: 196 \t reward: -135.07361200703008\n",
            "Episode 540 \t avg length: 200 \t reward: -136.51502587950478\n",
            "Episode 560 \t avg length: 179 \t reward: -134.96147657372848\n",
            "Episode 580 \t avg length: 254 \t reward: -130.17983643240052\n",
            "Episode 600 \t avg length: 369 \t reward: -80.27737682688301\n",
            "Episode 620 \t avg length: 178 \t reward: -119.76652228424159\n",
            "Episode 640 \t avg length: 20 \t reward: -104.72460064013269\n",
            "Episode 660 \t avg length: 28 \t reward: -105.03309349236879\n",
            "Episode 680 \t avg length: 167 \t reward: -118.28134021972578\n",
            "Episode 700 \t avg length: 216 \t reward: -137.28660019548258\n",
            "Episode 720 \t avg length: 171 \t reward: -134.46131168057104\n",
            "Episode 740 \t avg length: 197 \t reward: -139.09357615395857\n",
            "Episode 760 \t avg length: 172 \t reward: -134.63754103786934\n",
            "Episode 780 \t avg length: 197 \t reward: -139.03478012358153\n",
            "Episode 800 \t avg length: 191 \t reward: -135.22840231052587\n",
            "Episode 820 \t avg length: 176 \t reward: -133.18709679834703\n",
            "Episode 840 \t avg length: 179 \t reward: -133.74010246868906\n",
            "Episode 860 \t avg length: 195 \t reward: -136.90894401812722\n",
            "Episode 880 \t avg length: 196 \t reward: -136.54007534326155\n",
            "Episode 900 \t avg length: 209 \t reward: -137.33117842918824\n",
            "Episode 920 \t avg length: 190 \t reward: -137.0916585589436\n",
            "Episode 940 \t avg length: 191 \t reward: -133.35835107612616\n",
            "Episode 960 \t avg length: 69 \t reward: -110.7781736308516\n",
            "Episode 980 \t avg length: 174 \t reward: -127.39607273781262\n",
            "Episode 1000 \t avg length: 209 \t reward: -138.58839697546577\n",
            "Episode 1020 \t avg length: 71 \t reward: -111.97413111080618\n",
            "Episode 1040 \t avg length: 22 \t reward: -104.55396233703857\n",
            "Episode 1060 \t avg length: 40 \t reward: -106.70015004282827\n",
            "Episode 1080 \t avg length: 136 \t reward: -119.17898712174379\n",
            "Episode 1100 \t avg length: 46 \t reward: -108.06473128307084\n",
            "Episode 1120 \t avg length: 15 \t reward: -103.97309790599726\n",
            "Episode 1140 \t avg length: 20 \t reward: -104.64012072156802\n",
            "Episode 1160 \t avg length: 18 \t reward: -104.42211061072699\n",
            "Episode 1180 \t avg length: 17 \t reward: -104.40577761343157\n",
            "Episode 1200 \t avg length: 17 \t reward: -104.3416949261638\n",
            "Episode 1220 \t avg length: 18 \t reward: -104.69053413342617\n",
            "Episode 1240 \t avg length: 16 \t reward: -104.41601917238617\n",
            "Episode 1260 \t avg length: 17 \t reward: -104.56327853861774\n",
            "Episode 1280 \t avg length: 15 \t reward: -104.16592558626198\n",
            "Episode 1300 \t avg length: 17 \t reward: -104.17671587331236\n",
            "Episode 1320 \t avg length: 17 \t reward: -104.2918693654597\n",
            "Episode 1340 \t avg length: 17 \t reward: -104.29832903485863\n",
            "Episode 1360 \t avg length: 18 \t reward: -104.38962357633666\n",
            "Episode 1380 \t avg length: 15 \t reward: -104.06986508224209\n",
            "Episode 1400 \t avg length: 16 \t reward: -104.25132504812454\n",
            "Episode 1420 \t avg length: 18 \t reward: -104.72880538946784\n",
            "Episode 1440 \t avg length: 17 \t reward: -104.14591106655436\n",
            "Episode 1460 \t avg length: 16 \t reward: -104.51997695414855\n",
            "Episode 1480 \t avg length: 14 \t reward: -104.06863236016095\n",
            "Episode 1500 \t avg length: 17 \t reward: -104.48161352729373\n",
            "Episode 1520 \t avg length: 15 \t reward: -103.91403972357814\n",
            "Episode 1540 \t avg length: 18 \t reward: -104.67567344210269\n",
            "Episode 1560 \t avg length: 18 \t reward: -104.74960225776533\n",
            "Episode 1580 \t avg length: 15 \t reward: -104.19305517149003\n",
            "Episode 1600 \t avg length: 18 \t reward: -104.41815878040362\n",
            "Episode 1620 \t avg length: 17 \t reward: -104.0749528989684\n",
            "Episode 1640 \t avg length: 17 \t reward: -104.30613027399129\n",
            "Episode 1660 \t avg length: 17 \t reward: -104.29043173095356\n",
            "Episode 1680 \t avg length: 14 \t reward: -103.86216007798643\n",
            "Episode 1700 \t avg length: 17 \t reward: -104.46564060498702\n",
            "Episode 1720 \t avg length: 16 \t reward: -104.12657759206895\n",
            "Episode 1740 \t avg length: 17 \t reward: -104.05336595514272\n",
            "Episode 1760 \t avg length: 18 \t reward: -104.64047604474995\n",
            "Episode 1780 \t avg length: 18 \t reward: -104.95738211250323\n",
            "Episode 1800 \t avg length: 17 \t reward: -104.32991945127517\n",
            "Episode 1820 \t avg length: 15 \t reward: -103.76927516666815\n",
            "Episode 1840 \t avg length: 17 \t reward: -104.45391416399193\n",
            "Episode 1860 \t avg length: 18 \t reward: -104.37849743212732\n",
            "Episode 1880 \t avg length: 15 \t reward: -103.90772090851048\n",
            "Episode 1900 \t avg length: 17 \t reward: -104.27361478875086\n",
            "Episode 1920 \t avg length: 18 \t reward: -104.3585969509802\n",
            "Episode 1940 \t avg length: 124 \t reward: -118.88676751680892\n",
            "Episode 1960 \t avg length: 175 \t reward: -124.62754116543269\n",
            "Episode 1980 \t avg length: 203 \t reward: -135.8421858583938\n",
            "Episode 2000 \t avg length: 203 \t reward: -130.77184892748102\n",
            "Episode 2020 \t avg length: 197 \t reward: -133.06139392708917\n",
            "Episode 2040 \t avg length: 228 \t reward: -135.8939163922949\n",
            "Episode 2060 \t avg length: 193 \t reward: -132.26801199024806\n",
            "Episode 2080 \t avg length: 190 \t reward: -132.0773471639863\n",
            "Episode 2100 \t avg length: 196 \t reward: -133.55014284723683\n",
            "Episode 2120 \t avg length: 174 \t reward: -134.5643327710372\n",
            "Episode 2140 \t avg length: 184 \t reward: -135.33364594604808\n",
            "Episode 2160 \t avg length: 188 \t reward: -136.87229801256385\n",
            "Episode 2180 \t avg length: 185 \t reward: -137.05369360460244\n",
            "Episode 2200 \t avg length: 180 \t reward: -136.70307188614282\n",
            "Episode 2220 \t avg length: 204 \t reward: -138.78811741245124\n",
            "Episode 2240 \t avg length: 187 \t reward: -137.01725855890598\n",
            "Episode 2260 \t avg length: 179 \t reward: -135.23070714762505\n",
            "Episode 2280 \t avg length: 197 \t reward: -134.54110088440785\n",
            "Episode 2300 \t avg length: 162 \t reward: -131.19617834067668\n",
            "Episode 2320 \t avg length: 198 \t reward: -138.4486097222924\n",
            "Episode 2340 \t avg length: 201 \t reward: -137.18584802563421\n",
            "Episode 2360 \t avg length: 194 \t reward: -135.64318037809053\n",
            "Episode 2380 \t avg length: 211 \t reward: -135.88935878141967\n",
            "Episode 2400 \t avg length: 175 \t reward: -134.57145432299936\n",
            "Episode 2420 \t avg length: 158 \t reward: -131.04134792212724\n",
            "Episode 2440 \t avg length: 170 \t reward: -133.3096237674979\n",
            "Episode 2460 \t avg length: 191 \t reward: -136.65404180498166\n",
            "Episode 2480 \t avg length: 188 \t reward: -135.1415596236719\n",
            "Episode 2500 \t avg length: 171 \t reward: -133.000272409763\n",
            "Episode 2520 \t avg length: 195 \t reward: -136.8610583965647\n",
            "Episode 2540 \t avg length: 190 \t reward: -133.1046547587377\n",
            "Episode 2560 \t avg length: 184 \t reward: -133.1610462987959\n",
            "Episode 2580 \t avg length: 238 \t reward: -139.05497028730306\n",
            "Episode 2600 \t avg length: 255 \t reward: -133.61899381152193\n",
            "Episode 2620 \t avg length: 261 \t reward: -94.0928040846596\n",
            "Episode 2640 \t avg length: 192 \t reward: -84.2083635122245\n",
            "Episode 2660 \t avg length: 248 \t reward: -96.49341586055216\n",
            "Episode 2680 \t avg length: 209 \t reward: -80.90618017423392\n",
            "Episode 2700 \t avg length: 230 \t reward: -75.25292129874776\n",
            "Episode 2720 \t avg length: 150 \t reward: -84.05018122000611\n",
            "Episode 2740 \t avg length: 186 \t reward: -93.1738135857035\n",
            "Episode 2760 \t avg length: 266 \t reward: -77.12532666058243\n",
            "Episode 2780 \t avg length: 183 \t reward: -88.94713998397226\n",
            "Episode 2800 \t avg length: 144 \t reward: -96.55587804836401\n",
            "Episode 2820 \t avg length: 261 \t reward: -97.84125826358611\n",
            "Episode 2840 \t avg length: 303 \t reward: -78.64385542747613\n",
            "Episode 2860 \t avg length: 305 \t reward: -76.70118122291885\n",
            "Episode 2880 \t avg length: 329 \t reward: -67.02852150033456\n",
            "Episode 2900 \t avg length: 259 \t reward: -78.51686154578479\n",
            "Episode 2920 \t avg length: 281 \t reward: -69.36345520339623\n",
            "Episode 2940 \t avg length: 259 \t reward: -79.13928810584258\n",
            "Episode 2960 \t avg length: 268 \t reward: -77.63903598211682\n",
            "Episode 2980 \t avg length: 199 \t reward: -86.12077362171897\n",
            "Episode 3000 \t avg length: 266 \t reward: -90.20102220451516\n",
            "Episode 3020 \t avg length: 305 \t reward: -71.0020368353915\n",
            "Episode 3040 \t avg length: 235 \t reward: -82.16586547226595\n",
            "Episode 3060 \t avg length: 209 \t reward: -82.79240113268175\n",
            "Episode 3080 \t avg length: 167 \t reward: -86.78144974236301\n",
            "Episode 3100 \t avg length: 235 \t reward: -84.07277848108654\n",
            "Episode 3120 \t avg length: 133 \t reward: -96.67045644730784\n",
            "Episode 3140 \t avg length: 253 \t reward: -73.47429972590228\n",
            "Episode 3160 \t avg length: 226 \t reward: -78.674473474441\n",
            "Episode 3180 \t avg length: 202 \t reward: -83.60282637247934\n",
            "Episode 3200 \t avg length: 189 \t reward: -81.97603012293668\n",
            "Episode 3220 \t avg length: 184 \t reward: -100.95960475733357\n",
            "Episode 3240 \t avg length: 196 \t reward: -86.5408272255522\n",
            "Episode 3260 \t avg length: 229 \t reward: -75.35620643101649\n",
            "Episode 3280 \t avg length: 229 \t reward: -87.36829897361676\n",
            "Episode 3300 \t avg length: 169 \t reward: -89.37628923855289\n",
            "Episode 3320 \t avg length: 249 \t reward: -82.5864539028107\n",
            "Episode 3340 \t avg length: 227 \t reward: -80.33741545067093\n",
            "Episode 3360 \t avg length: 267 \t reward: -72.78950457720423\n",
            "Episode 3380 \t avg length: 237 \t reward: -80.40439871217986\n",
            "Episode 3400 \t avg length: 178 \t reward: -87.85910478841856\n",
            "Episode 3420 \t avg length: 196 \t reward: -90.15115394377699\n",
            "Episode 3440 \t avg length: 306 \t reward: -78.9092504151839\n",
            "Episode 3460 \t avg length: 232 \t reward: -84.36094919692447\n",
            "Episode 3480 \t avg length: 236 \t reward: -92.9456622740203\n",
            "Episode 3500 \t avg length: 206 \t reward: -83.44041545030876\n",
            "Episode 3520 \t avg length: 294 \t reward: -78.41325722128089\n",
            "Episode 3540 \t avg length: 259 \t reward: -68.36261259632627\n",
            "Episode 3560 \t avg length: 142 \t reward: -90.64577083110063\n",
            "Episode 3580 \t avg length: 231 \t reward: -86.78816002419413\n",
            "Episode 3600 \t avg length: 325 \t reward: -66.14211130024657\n",
            "Episode 3620 \t avg length: 218 \t reward: -74.43313891152711\n",
            "Episode 3640 \t avg length: 176 \t reward: -88.37858986806371\n",
            "Episode 3660 \t avg length: 325 \t reward: -72.08929460693001\n",
            "Episode 3680 \t avg length: 319 \t reward: -65.86939253920085\n",
            "Episode 3700 \t avg length: 181 \t reward: -89.93931384506713\n",
            "Episode 3720 \t avg length: 164 \t reward: -94.49954572571028\n",
            "Episode 3740 \t avg length: 332 \t reward: -86.8583699237142\n",
            "Episode 3760 \t avg length: 216 \t reward: -80.66075246137012\n",
            "Episode 3780 \t avg length: 186 \t reward: -78.15301986663368\n",
            "Episode 3800 \t avg length: 267 \t reward: -88.34095521195862\n",
            "Episode 3820 \t avg length: 246 \t reward: -76.95418657213551\n",
            "Episode 3840 \t avg length: 212 \t reward: -80.04119357464815\n",
            "Episode 3860 \t avg length: 263 \t reward: -86.64738029845326\n",
            "Episode 3880 \t avg length: 287 \t reward: -79.23282236423886\n",
            "Episode 3900 \t avg length: 255 \t reward: -78.76809117130577\n",
            "Episode 3920 \t avg length: 257 \t reward: -75.74847314928017\n",
            "Episode 3940 \t avg length: 238 \t reward: -77.34043101701221\n",
            "Episode 3960 \t avg length: 202 \t reward: -85.97580226640287\n",
            "Episode 3980 \t avg length: 360 \t reward: -67.67349655652936\n",
            "Episode 4000 \t avg length: 177 \t reward: -99.44949325444958\n",
            "Episode 4020 \t avg length: 331 \t reward: -66.59705977576638\n",
            "Episode 4040 \t avg length: 257 \t reward: -83.08655991186164\n",
            "Episode 4060 \t avg length: 276 \t reward: -86.20709232631485\n",
            "Episode 4080 \t avg length: 248 \t reward: -85.47065023707556\n",
            "Episode 4100 \t avg length: 212 \t reward: -81.68239075644622\n",
            "Episode 4120 \t avg length: 281 \t reward: -74.64076844599126\n",
            "Episode 4140 \t avg length: 340 \t reward: -67.63577516290002\n",
            "Episode 4160 \t avg length: 329 \t reward: -63.9839591567674\n",
            "Episode 4180 \t avg length: 237 \t reward: -87.60493754846154\n",
            "Episode 4200 \t avg length: 249 \t reward: -78.15122827208317\n",
            "Episode 4220 \t avg length: 228 \t reward: -80.77829681431827\n",
            "Episode 4240 \t avg length: 254 \t reward: -73.03059734833356\n",
            "Episode 4260 \t avg length: 282 \t reward: -74.92107612458928\n",
            "Episode 4280 \t avg length: 235 \t reward: -87.64092096575965\n",
            "Episode 4300 \t avg length: 296 \t reward: -68.29592276385989\n",
            "Episode 4320 \t avg length: 196 \t reward: -79.80246700606354\n",
            "Episode 4340 \t avg length: 228 \t reward: -82.63694171169486\n",
            "Episode 4360 \t avg length: 248 \t reward: -73.0610010506644\n",
            "Episode 4380 \t avg length: 303 \t reward: -79.61152679686852\n",
            "Episode 4400 \t avg length: 266 \t reward: -71.05551930217305\n",
            "Episode 4420 \t avg length: 309 \t reward: -74.5222927922639\n",
            "Episode 4440 \t avg length: 370 \t reward: -55.35616119982202\n",
            "Episode 4460 \t avg length: 134 \t reward: -91.82210700961676\n",
            "Episode 4480 \t avg length: 234 \t reward: -77.23674443624927\n",
            "Episode 4500 \t avg length: 220 \t reward: -85.987556944064\n",
            "Episode 4520 \t avg length: 266 \t reward: -85.05352555596403\n",
            "Episode 4540 \t avg length: 199 \t reward: -84.03981678890338\n",
            "Episode 4560 \t avg length: 231 \t reward: -85.20170680861646\n",
            "Episode 4580 \t avg length: 176 \t reward: -85.03223684786724\n",
            "Episode 4600 \t avg length: 219 \t reward: -75.04068180790001\n",
            "Episode 4620 \t avg length: 218 \t reward: -92.0211110796402\n",
            "Episode 4640 \t avg length: 153 \t reward: -93.42997801266807\n",
            "Episode 4660 \t avg length: 178 \t reward: -83.50939569310654\n",
            "Episode 4680 \t avg length: 173 \t reward: -89.02808723782454\n",
            "Episode 4700 \t avg length: 248 \t reward: -81.06553964816797\n",
            "Episode 4720 \t avg length: 278 \t reward: -74.74484764940217\n",
            "Episode 4740 \t avg length: 222 \t reward: -89.39107872545469\n",
            "Episode 4760 \t avg length: 303 \t reward: -67.55635533211866\n",
            "Episode 4780 \t avg length: 193 \t reward: -83.63610180449476\n",
            "Episode 4800 \t avg length: 331 \t reward: -62.36417837977458\n",
            "Episode 4820 \t avg length: 200 \t reward: -90.44820121869263\n",
            "Episode 4840 \t avg length: 201 \t reward: -84.34051025903368\n",
            "Episode 4860 \t avg length: 209 \t reward: -73.99338641295883\n",
            "Episode 4880 \t avg length: 198 \t reward: -83.01402579136376\n",
            "Episode 4900 \t avg length: 182 \t reward: -94.5092673284401\n",
            "Episode 4920 \t avg length: 256 \t reward: -74.66603873498778\n",
            "Episode 4940 \t avg length: 236 \t reward: -79.53522763853971\n",
            "Episode 4960 \t avg length: 168 \t reward: -82.26807975341188\n",
            "Episode 4980 \t avg length: 295 \t reward: -76.04121246356523\n",
            "Episode 5000 \t avg length: 170 \t reward: -88.3251219565409\n",
            "Episode 5020 \t avg length: 282 \t reward: -80.12961221555636\n",
            "Episode 5040 \t avg length: 253 \t reward: -72.97862389646805\n",
            "Episode 5060 \t avg length: 176 \t reward: -106.00572036303359\n",
            "Episode 5080 \t avg length: 269 \t reward: -74.20451139522167\n",
            "Episode 5100 \t avg length: 243 \t reward: -77.24224089009876\n",
            "Episode 5120 \t avg length: 271 \t reward: -74.88822986152358\n",
            "Episode 5140 \t avg length: 248 \t reward: -84.54009887236786\n",
            "Episode 5160 \t avg length: 227 \t reward: -75.27846418034501\n",
            "Episode 5180 \t avg length: 237 \t reward: -84.27357791479571\n",
            "Episode 5200 \t avg length: 229 \t reward: -93.1936438928461\n",
            "Episode 5220 \t avg length: 137 \t reward: -95.41850897078345\n",
            "Episode 5240 \t avg length: 272 \t reward: -79.38245045206962\n",
            "Episode 5260 \t avg length: 325 \t reward: -67.13666332344124\n",
            "Episode 5280 \t avg length: 236 \t reward: -96.54605975401279\n",
            "Episode 5300 \t avg length: 210 \t reward: -82.2511499276941\n",
            "Episode 5320 \t avg length: 249 \t reward: -82.52476515115943\n",
            "Episode 5340 \t avg length: 279 \t reward: -79.42187122924446\n",
            "Episode 5360 \t avg length: 380 \t reward: -75.89746832955845\n",
            "Episode 5380 \t avg length: 252 \t reward: -70.01810806823654\n",
            "Episode 5400 \t avg length: 238 \t reward: -69.66937280497545\n",
            "Episode 5420 \t avg length: 274 \t reward: -82.80831083836546\n",
            "Episode 5440 \t avg length: 258 \t reward: -68.34425651958217\n",
            "Episode 5460 \t avg length: 212 \t reward: -79.80331302311333\n",
            "Episode 5480 \t avg length: 171 \t reward: -88.41950469763461\n",
            "Episode 5500 \t avg length: 260 \t reward: -79.34740484839794\n",
            "Episode 5520 \t avg length: 233 \t reward: -73.47884917937395\n",
            "Episode 5540 \t avg length: 340 \t reward: -60.99331182574804\n",
            "Episode 5560 \t avg length: 155 \t reward: -90.98085470855466\n",
            "Episode 5580 \t avg length: 187 \t reward: -99.78882591020097\n",
            "Episode 5600 \t avg length: 224 \t reward: -87.3882706808466\n",
            "Episode 5620 \t avg length: 268 \t reward: -73.78740180339786\n",
            "Episode 5640 \t avg length: 274 \t reward: -84.35839361306016\n",
            "Episode 5660 \t avg length: 207 \t reward: -92.06584825499095\n",
            "Episode 5680 \t avg length: 181 \t reward: -93.062182518877\n",
            "Episode 5700 \t avg length: 216 \t reward: -86.21875463632544\n",
            "Episode 5720 \t avg length: 224 \t reward: -89.18777532071626\n",
            "Episode 5740 \t avg length: 173 \t reward: -87.99497743723616\n",
            "Episode 5760 \t avg length: 135 \t reward: -95.13471039083886\n",
            "Episode 5780 \t avg length: 141 \t reward: -91.62525936341964\n",
            "Episode 5800 \t avg length: 261 \t reward: -84.23721170648179\n",
            "Episode 5820 \t avg length: 264 \t reward: -75.81244173517244\n",
            "Episode 5840 \t avg length: 262 \t reward: -68.95144281807174\n",
            "Episode 5860 \t avg length: 299 \t reward: -80.90255996062089\n",
            "Episode 5880 \t avg length: 293 \t reward: -75.74652215810954\n",
            "Episode 5900 \t avg length: 209 \t reward: -80.10085966579172\n",
            "Episode 5920 \t avg length: 223 \t reward: -82.6663192279523\n",
            "Episode 5940 \t avg length: 248 \t reward: -76.90884564876501\n",
            "Episode 5960 \t avg length: 310 \t reward: -63.7088884459925\n",
            "Episode 5980 \t avg length: 288 \t reward: -74.34467982547054\n",
            "Episode 6000 \t avg length: 249 \t reward: -81.2025482004853\n",
            "Episode 6020 \t avg length: 236 \t reward: -76.42359787576389\n",
            "Episode 6040 \t avg length: 273 \t reward: -71.03433409461486\n",
            "Episode 6060 \t avg length: 160 \t reward: -86.13873991641887\n",
            "Episode 6080 \t avg length: 225 \t reward: -87.02400077558259\n",
            "Episode 6100 \t avg length: 190 \t reward: -84.25752851759782\n",
            "Episode 6120 \t avg length: 286 \t reward: -87.35108309900717\n",
            "Episode 6140 \t avg length: 277 \t reward: -85.50130990666291\n",
            "Episode 6160 \t avg length: 244 \t reward: -87.01809256569487\n",
            "Episode 6180 \t avg length: 255 \t reward: -73.72321371060175\n",
            "Episode 6200 \t avg length: 248 \t reward: -82.31199655344855\n",
            "Episode 6220 \t avg length: 279 \t reward: -87.53522751108387\n",
            "Episode 6240 \t avg length: 293 \t reward: -85.52639588559167\n",
            "Episode 6260 \t avg length: 239 \t reward: -83.29226808349073\n",
            "Episode 6280 \t avg length: 223 \t reward: -82.60186859962815\n",
            "Episode 6300 \t avg length: 275 \t reward: -79.87242309672229\n",
            "Episode 6320 \t avg length: 230 \t reward: -80.95751722909455\n",
            "Episode 6340 \t avg length: 169 \t reward: -92.05773771417478\n",
            "Episode 6360 \t avg length: 226 \t reward: -96.69627851114703\n",
            "Episode 6380 \t avg length: 131 \t reward: -95.21998368942278\n",
            "Episode 6400 \t avg length: 236 \t reward: -81.19732703144533\n",
            "Episode 6420 \t avg length: 249 \t reward: -89.45220944762227\n",
            "Episode 6440 \t avg length: 277 \t reward: -75.21392704692451\n",
            "Episode 6460 \t avg length: 204 \t reward: -85.36240216958669\n",
            "Episode 6480 \t avg length: 272 \t reward: -67.33774744204526\n",
            "Episode 6500 \t avg length: 184 \t reward: -87.76833295715413\n",
            "Episode 6520 \t avg length: 274 \t reward: -65.06062780065129\n",
            "Episode 6540 \t avg length: 142 \t reward: -86.80883885552424\n",
            "Episode 6560 \t avg length: 178 \t reward: -105.32385323328307\n",
            "Episode 6580 \t avg length: 251 \t reward: -75.61865450024075\n",
            "Episode 6600 \t avg length: 184 \t reward: -95.29896587572398\n",
            "Episode 6620 \t avg length: 216 \t reward: -85.12929068565134\n",
            "Episode 6640 \t avg length: 225 \t reward: -81.7525760666033\n",
            "Episode 6660 \t avg length: 222 \t reward: -93.63197542727296\n",
            "Episode 6680 \t avg length: 206 \t reward: -92.2216210240657\n",
            "Episode 6700 \t avg length: 331 \t reward: -62.206082206084396\n",
            "Episode 6720 \t avg length: 303 \t reward: -65.120353544289\n",
            "Episode 6740 \t avg length: 286 \t reward: -77.79099129573137\n",
            "Episode 6760 \t avg length: 233 \t reward: -86.68089198396716\n",
            "Episode 6780 \t avg length: 280 \t reward: -79.17691266427073\n",
            "Episode 6800 \t avg length: 228 \t reward: -80.56308634138433\n",
            "Episode 6820 \t avg length: 276 \t reward: -80.99013585061911\n",
            "Episode 6840 \t avg length: 223 \t reward: -92.68155035263837\n",
            "Episode 6860 \t avg length: 323 \t reward: -69.00605050899705\n",
            "Episode 6880 \t avg length: 234 \t reward: -75.90137581614457\n",
            "Episode 6900 \t avg length: 290 \t reward: -80.73383922512792\n",
            "Episode 6920 \t avg length: 313 \t reward: -63.60484674573934\n",
            "Episode 6940 \t avg length: 261 \t reward: -78.71288313842044\n",
            "Episode 6960 \t avg length: 221 \t reward: -84.20872506763058\n",
            "Episode 6980 \t avg length: 212 \t reward: -79.52113647308384\n",
            "Episode 7000 \t avg length: 172 \t reward: -87.44467751868646\n",
            "Episode 7020 \t avg length: 255 \t reward: -72.84908559523137\n",
            "Episode 7040 \t avg length: 255 \t reward: -75.75043764752424\n",
            "Episode 7060 \t avg length: 265 \t reward: -78.42385828824885\n",
            "Episode 7080 \t avg length: 141 \t reward: -102.21967983432306\n",
            "Episode 7100 \t avg length: 336 \t reward: -62.285734855772965\n",
            "Episode 7120 \t avg length: 221 \t reward: -75.85391764796043\n",
            "Episode 7140 \t avg length: 361 \t reward: -64.65554135894793\n",
            "Episode 7160 \t avg length: 147 \t reward: -85.72656727651402\n",
            "Episode 7180 \t avg length: 155 \t reward: -92.83099901114197\n",
            "Episode 7200 \t avg length: 338 \t reward: -63.49490310471164\n",
            "Episode 7220 \t avg length: 136 \t reward: -85.9842809724295\n",
            "Episode 7240 \t avg length: 247 \t reward: -83.11178374254254\n",
            "Episode 7260 \t avg length: 216 \t reward: -86.5111615340555\n",
            "Episode 7280 \t avg length: 353 \t reward: -56.375742406739526\n",
            "Episode 7300 \t avg length: 279 \t reward: -72.47270812916096\n",
            "Episode 7320 \t avg length: 172 \t reward: -89.11393220116662\n",
            "Episode 7340 \t avg length: 295 \t reward: -86.38762546249225\n",
            "Episode 7360 \t avg length: 197 \t reward: -96.76588654116965\n",
            "Episode 7380 \t avg length: 342 \t reward: -67.87607207032286\n",
            "Episode 7400 \t avg length: 198 \t reward: -87.03741388622797\n",
            "Episode 7420 \t avg length: 234 \t reward: -79.32671578325306\n",
            "Episode 7440 \t avg length: 224 \t reward: -86.52915345613526\n",
            "Episode 7460 \t avg length: 197 \t reward: -78.39185980424398\n",
            "Episode 7480 \t avg length: 281 \t reward: -63.75038620006781\n",
            "Episode 7500 \t avg length: 240 \t reward: -78.21328857183745\n",
            "Episode 7520 \t avg length: 185 \t reward: -95.37344672127271\n",
            "Episode 7540 \t avg length: 262 \t reward: -69.20121777685343\n",
            "Episode 7560 \t avg length: 277 \t reward: -70.20427694926956\n",
            "Episode 7580 \t avg length: 308 \t reward: -75.5587392960517\n",
            "Episode 7600 \t avg length: 249 \t reward: -95.81426590708634\n",
            "Episode 7620 \t avg length: 140 \t reward: -91.20050154578479\n",
            "Episode 7640 \t avg length: 263 \t reward: -74.20735979644803\n",
            "Episode 7660 \t avg length: 317 \t reward: -67.38426551506618\n",
            "Episode 7680 \t avg length: 177 \t reward: -88.50014817800576\n",
            "Episode 7700 \t avg length: 190 \t reward: -83.42897765038303\n",
            "Episode 7720 \t avg length: 293 \t reward: -78.2034535401588\n",
            "Episode 7740 \t avg length: 293 \t reward: -82.33596883346642\n",
            "Episode 7760 \t avg length: 257 \t reward: -82.13572217631494\n",
            "Episode 7780 \t avg length: 278 \t reward: -80.58118794399346\n",
            "Episode 7800 \t avg length: 290 \t reward: -74.44694589246535\n",
            "Episode 7820 \t avg length: 226 \t reward: -75.9311098984688\n",
            "Episode 7840 \t avg length: 181 \t reward: -84.91549048787267\n",
            "Episode 7860 \t avg length: 232 \t reward: -81.68653846839834\n",
            "Episode 7880 \t avg length: 233 \t reward: -82.0485152019259\n",
            "Episode 7900 \t avg length: 284 \t reward: -80.06855964511996\n",
            "Episode 7920 \t avg length: 305 \t reward: -69.8867747700654\n",
            "Episode 7940 \t avg length: 233 \t reward: -69.36892628325066\n",
            "Episode 7960 \t avg length: 333 \t reward: -74.28624757741859\n",
            "Episode 7980 \t avg length: 326 \t reward: -60.85172135017008\n",
            "Episode 8000 \t avg length: 295 \t reward: -70.23551110731145\n",
            "Episode 8020 \t avg length: 325 \t reward: -70.51836937554005\n",
            "Episode 8040 \t avg length: 200 \t reward: -77.0570078035403\n",
            "Episode 8060 \t avg length: 198 \t reward: -83.38572742479688\n",
            "Episode 8080 \t avg length: 211 \t reward: -90.67886616005435\n",
            "Episode 8100 \t avg length: 179 \t reward: -88.8269762370554\n",
            "Episode 8120 \t avg length: 252 \t reward: -75.82275926493784\n",
            "Episode 8140 \t avg length: 256 \t reward: -82.76482216224863\n",
            "Episode 8160 \t avg length: 281 \t reward: -80.92165129975763\n",
            "Episode 8180 \t avg length: 182 \t reward: -84.94999969966744\n",
            "Episode 8200 \t avg length: 259 \t reward: -82.60044961801971\n",
            "Episode 8220 \t avg length: 234 \t reward: -77.96321980545031\n",
            "Episode 8240 \t avg length: 258 \t reward: -89.94976700946238\n",
            "Episode 8260 \t avg length: 185 \t reward: -89.3601834512833\n",
            "Episode 8280 \t avg length: 251 \t reward: -76.20692227849331\n",
            "Episode 8300 \t avg length: 249 \t reward: -101.98888632541781\n",
            "Episode 8320 \t avg length: 154 \t reward: -92.52523562673056\n",
            "Episode 8340 \t avg length: 219 \t reward: -73.55517056055096\n",
            "Episode 8360 \t avg length: 262 \t reward: -88.23431565038346\n",
            "Episode 8380 \t avg length: 266 \t reward: -76.01344729490044\n",
            "Episode 8400 \t avg length: 309 \t reward: -70.84307061745338\n",
            "Episode 8420 \t avg length: 284 \t reward: -77.66181317065718\n",
            "Episode 8440 \t avg length: 235 \t reward: -75.6925212277909\n",
            "Episode 8460 \t avg length: 240 \t reward: -70.2217005844737\n",
            "Episode 8480 \t avg length: 243 \t reward: -87.9712334811773\n",
            "Episode 8500 \t avg length: 239 \t reward: -76.40933157674695\n",
            "Episode 8520 \t avg length: 320 \t reward: -67.3082182490937\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3bca163389d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# train from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0micfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInitialCondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mhistory\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_condition_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0micfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# load from pth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-f4cc660d35e9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, initial_condition, initial_condition_fn, episodes)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Running policy_old:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_old\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-f4cc660d35e9>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, memory)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.test_initial_conditions(InitialCondition(distance=dd, f2=1.2) for dd in range(20, 65, 10))"
      ],
      "metadata": {
        "id": "h8dSJB6F76DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the policy\n",
        "# fa_range = numpy.linspace(-10, 10, 100)\n",
        "# f2_range = numpy.linspace(0.5, 1.5, 100)\n",
        "# a2_range = numpy.linspace(0, 3, 100)\n",
        "# d_range = numpy.linspace(0, 50, 100)\n",
        "# u_range = numpy.linspace(0, 30, 100)\n",
        "# p_range = numpy.linspace(0, 3, 100)\n",
        "# trainer.plot_policy(\n",
        "#     fa_range=fa_range,\n",
        "#     f2_range=f2_range,\n",
        "#     a2_range=a2_range,\n",
        "#     d_range=d_range,\n",
        "#     u_range=u_range,\n",
        "#     p_range=p_range\n",
        "# )"
      ],
      "metadata": {
        "id": "ksMYd2CQ5zeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.create_video(time=20, ic=InitialCondition(distance=30, A2=2.5))"
      ],
      "metadata": {
        "id": "x4_YXH-xnyX9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}