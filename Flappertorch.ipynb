{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorbuenog/Flapper/blob/main/Flappertorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "import numpy\n",
        "import scipy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.path as mpath\n",
        "from matplotlib.markers import MarkerStyle\n",
        "import matplotlib.font_manager as fm\n",
        "plt.ioff()\n",
        "\n",
        "import gym\n",
        "from gym import wrappers\n",
        "from gym import spaces\n",
        "\n",
        "import imageio"
      ],
      "metadata": {
        "id": "V1tlyuUFgqtI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QuAwnp9LVMnk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c118fb5-e169-4b7c-8b66-29c8249460a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "class InitialCondition(object):\n",
        "\n",
        "    def __init__(self, distance=None, f2=None, A2=None, goal=None):\n",
        "        self.distance = distance if distance is not None else 21.5\n",
        "        self.A1 = 2.0\n",
        "        self.f1 = 1.0\n",
        "        self.A2 = 2.0 if A2 is None else A2\n",
        "        self.f2 = 1.0 if f2 is None else f2\n",
        "        self.goal = goal if goal is not None else 21.5\n",
        "\n",
        "        self.u2 = numpy.pi * self.A2 * self.f2 * numpy.sqrt(2 * SwimmerEnv.Ct / SwimmerEnv.Cd)\n",
        "        self.v2 = -self.A2*(2 * numpy.pi * self.f2)\n",
        "        self.v_flow = self.A1*self.f1*numpy.cos(2*numpy.pi*self.f1*(-SwimmerEnv.dt))*numpy.exp(-SwimmerEnv.dt/SwimmerEnv.T)\n",
        "        self.flow_agreement = self.v2 * self.v_flow\n",
        "\n",
        "    def random(self, randomize_fields=[]):\n",
        "        if 'distance' in randomize_fields:\n",
        "            self.distance = random.uniform(10,30)\n",
        "        if 'f2' in randomize_fields:\n",
        "            self.f2 = random.uniform(0.5, 1.5)\n",
        "        if 'A2' in randomize_fields:\n",
        "            self.A2 = random.uniform(.5, 3.0)\n",
        "        if 'v2' in randomize_fields:\n",
        "            self.v2 = random.uniform(-1.0, 1.0)\n",
        "        return self\n",
        "\n",
        "\n",
        "class SwimmerEnv(gym.Env):\n",
        "\n",
        "    s = 15.\n",
        "    c = 4.\n",
        "    As = s * c\n",
        "    T = .5\n",
        "    m = 80.\n",
        "    Ct = .96\n",
        "    Cd = .25\n",
        "    rho = 1.\n",
        "    dt = 0.1\n",
        "\n",
        "    def __init__(self, action=None, observations=[], rewards=[]):\n",
        "        super(SwimmerEnv, self).__init__()\n",
        "\n",
        "        self.action_space = spaces.Discrete(3) # discrete action as 0, 1, or 2\n",
        "        self.action = action  # None, 'f2', or 'A2'\n",
        "\n",
        "        obs_low, obs_high = [], []\n",
        "        if 'distance' in observations:\n",
        "            obs_low.append(-numpy.inf)\n",
        "            obs_high.append(numpy.inf)\n",
        "        if 'f2' in observations:\n",
        "            obs_low.append(0.)\n",
        "            obs_high.append(10.)\n",
        "        if 'A2' in observations:\n",
        "            obs_low.append(0.5)\n",
        "            obs_high.append(3.0)\n",
        "        if 'flow agreement' in observations:\n",
        "            obs_low.append(-numpy.inf)\n",
        "            obs_high.append(numpy.inf)\n",
        "        if 'avg flow agreement' in observations:\n",
        "            obs_low.append(-numpy.inf)\n",
        "            obs_high.append(numpy.inf)\n",
        "        if 'velocity' in observations:\n",
        "            obs_low.append(0.)\n",
        "            obs_high.append(numpy.inf)\n",
        "\n",
        "        self.observation_space = spaces.Box(low=numpy.array(obs_low), high=numpy.array(obs_high), dtype=numpy.float32)\n",
        "        self.observations = observations\n",
        "\n",
        "        self.rewards = rewards\n",
        "\n",
        "        self.flap = None\n",
        "        self.t_bound = 500.\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def _shoot(self, A1, A2, f1, f2, vec_initial, t_start=0., t_bound=5000., method='RK45'):\n",
        "        rho = self.rho\n",
        "        As = self.As\n",
        "        T = self.T\n",
        "        m = self.m\n",
        "        Ct = self.Ct\n",
        "        Cd = self.Cd\n",
        "        c = self.c\n",
        "\n",
        "        def fun(t, vec):\n",
        "            x2, u2 = vec\n",
        "            u1 = numpy.pi * A1 * f1 * numpy.sqrt(2 * Ct / Cd)  # Leader velocity (constant)\n",
        "            dt = -x2 / u1 + t\n",
        "            Ft2 = 2*rho*As*Ct*numpy.pi**2*((A2*f2*numpy.cos(2*numpy.pi*f2*t)-A1*f1*numpy.cos(2*numpy.pi*f1*(t-dt))*numpy.exp(-dt/T)))**2\n",
        "            dy_dt = (u2, (Ft2 - rho*As*Cd*u2**2/2)/m)\n",
        "            return numpy.asarray(dy_dt)\n",
        "        # events = [lambda t, y: y[0] - y[4] - 0.00001]\n",
        "        events = []\n",
        "        for ee in events: setattr(ee, 'terminal', True)\n",
        "        solver = scipy.integrate.solve_ivp(fun, (t_start, t_bound), vec_initial, method=method, events=events,\n",
        "                                            rtol=1e-4, atol=1e-7, max_step=.03, first_step=.001, dense_output=True)\n",
        "        u1 = numpy.pi * A1 * f1 * numpy.sqrt(2 * Ct / Cd)  # Leader velocity (constant)\n",
        "        x1 = u1 * solver.t[-1]  # Leader x-coord (calculated)\n",
        "        y1 = A1 * numpy.sin(2 * numpy.pi * f1 * solver.t[-1])  # Leader y-coord (calculated)\n",
        "        v1 = -A1*(2 * numpy.pi * f1)*numpy.cos(2 * numpy.pi * f1 * solver.t[-1])\n",
        "        y2 = A2 * numpy.sin(2 * numpy.pi * f2 * solver.t[-1])  # Leader y-coord (calculated)\n",
        "        v2 = -A2*(2 * numpy.pi * f2)*numpy.cos(2 * numpy.pi * f2 * solver.t[-1])\n",
        "        values = list(zip(solver.t, solver.y.T))\n",
        "\n",
        "        info = {\n",
        "            'x1': x1,\n",
        "            'y1': y1,\n",
        "            'u1': u1,\n",
        "            'v1': v1,\n",
        "            'x2': values[-1][1][0],\n",
        "            'y2': y2,\n",
        "            'u2': values[-1][1][0],\n",
        "            'v2': v2,\n",
        "        }\n",
        "        return solver, values, info\n",
        "\n",
        "\n",
        "    def _get_obs(self):\n",
        "        obs = []\n",
        "        if 'distance' in self.observations:\n",
        "            obs.append(self.distance)\n",
        "        if 'f2' in self.observations:\n",
        "            obs.append(self.f2)\n",
        "        if 'A2' in self.observations:\n",
        "            obs.append(self.A2)\n",
        "        if 'flow agreement' in self.observations:\n",
        "            obs.append(self.flow_agreement)\n",
        "        if 'avg flow agreement' in self.observations:\n",
        "            obs.append(self.avg_flow_agreement)\n",
        "        if 'velocity' in self.observations:\n",
        "            obs.append(self.u2)\n",
        "        obs = numpy.array(obs, dtype=numpy.float32)\n",
        "        return obs\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        Ct = self.Ct\n",
        "        Cd = self.Cd\n",
        "\n",
        "        increment = (action - 1) * .1\n",
        "        if self.action == 'f2':\n",
        "            self.f2 = self.f2 + increment\n",
        "        elif self.action == 'A2':\n",
        "            self.A2 = self.A2 + increment\n",
        "\n",
        "        t_bound_step = self.dt\n",
        "        solver, values, shoot_info = self._shoot(self.A1, self.A2, self.f1, self.f2, self.flap, t_start=self.tt, t_bound=self.tt+t_bound_step)\n",
        "        self.flap = values[-1][1]\n",
        "        self.tt += self.dt\n",
        "        self.u1 = numpy.pi * self.A1 * self.f1 * numpy.sqrt(2 * Ct / Cd)  # Leader velocity\n",
        "        self.x1 = self.u1 * self.tt  # Leader x-coord\n",
        "        self.y1 = self.A1 * numpy.sin(2 * numpy.pi * self.f1 * self.tt)  # Leader y-coord\n",
        "        self.y2 = self.A2 * numpy.sin(2 * numpy.pi * self.f2 * self.tt)  # Follower y-coord\n",
        "        self.v1 = -self.A1*(2 * numpy.pi * self.f1)*numpy.cos(2 * numpy.pi * self.f1 * self.tt)\n",
        "        self.t_delay = self.tt - self.flap[0]/self.u1\n",
        "        v_flow = self.A1*self.f1*numpy.cos(2*numpy.pi*self.f1*(self.tt-self.t_delay))*numpy.exp(-self.t_delay/self.T)\n",
        "        self.t_delay_head = self.tt - (self.flap[0]+self.c)/self.u1\n",
        "        v_flow_head = self.A1*self.f1*numpy.cos(2*numpy.pi*self.f1*(self.tt-self.t_delay_head))*numpy.exp(-self.t_delay_head/self.T)\n",
        "        v_gradient = (v_flow - v_flow_head)/self.c\n",
        "\n",
        "        self.distance = self.x1 - self.flap[0]-self.c  # Distance between leader and follower\n",
        "        done = False\n",
        "        new_distance_from_goal = numpy.abs(self.distance - self.goal) # Distance\n",
        "        new_flow_agreement = self.flap[1] * v_flow_head # Flow agreement\n",
        "        a_flow = (v_flow-self.previous_v_flow)/self.dt # Flow acceleration\n",
        "        self.u2 = (self.flap[0]-self.previous_x2)/self.dt # Follower velocity\n",
        "\n",
        "        # Calculate average flow agreement\n",
        "        self.flow_agreement_history.append(shoot_info['v2'] * v_flow)\n",
        "        if len(self.flow_agreement_history) > 50:\n",
        "            self.flow_agreement_history.pop(0)\n",
        "        new_avg_flow_agreement = numpy.mean(self.flow_agreement_history)\n",
        "\n",
        "        # Reset reward\n",
        "        self.reward = 0.\n",
        "\n",
        "        if 'distance' in self.rewards:\n",
        "            self.reward += (self.distance_from_goal - new_distance_from_goal)\n",
        "\n",
        "        if 'flow agreement' in self.rewards:\n",
        "            self.reward += (new_flow_agreement - self.flow_agreement)\n",
        "\n",
        "        if 'flow acceleration' in self.rewards:\n",
        "            self.reward += a_flow\n",
        "\n",
        "        if 'velocity' in self.rewards:\n",
        "            self.reward = -numpy.abs(self.u1-self.u2)\n",
        "\n",
        "        if 'avg flow agreement' in self.rewards:\n",
        "            self.reward += new_avg_flow_agreement\n",
        "\n",
        "        self.previous_v_flow = v_flow\n",
        "        self.distance_from_goal = new_distance_from_goal\n",
        "        self.flow_agreement = new_flow_agreement\n",
        "        self.avg_flow_agreement = new_avg_flow_agreement\n",
        "        self.previous_x2 = self.flap[0]\n",
        "        self.last_reward = self.reward\n",
        "\n",
        "        # Penalize extreme behaviors\n",
        "        if self.distance < 0.:\n",
        "            self.reward -= 100\n",
        "            done = True\n",
        "        elif self.distance > 200:\n",
        "            self.reward -= 100\n",
        "            done = True\n",
        "\n",
        "        info = {\n",
        "            'distance': self.distance,\n",
        "            'action': action,\n",
        "            'reward': self.reward,\n",
        "            'done': done,\n",
        "            'freq1': self.f1,\n",
        "            'freq2': self.f2,\n",
        "            'distance_from_goal': self.distance_from_goal,\n",
        "            't': self.tt,\n",
        "            'v_flow': v_flow,\n",
        "            'v_flow_head': v_flow_head,\n",
        "            'a_flow': a_flow,\n",
        "            'flow_agreement': self.flow_agreement,\n",
        "            'avg_flow_agreement': self.avg_flow_agreement,\n",
        "            'u2': self.flap[1],\n",
        "            'v_gradient': v_gradient,\n",
        "        }\n",
        "        info.update(shoot_info)\n",
        "\n",
        "        return self._get_obs(), self.reward, done, info\n",
        "\n",
        "    def reset(self, initial_condition=None, initial_condition_fn=None):\n",
        "        Ct = self.Ct\n",
        "        Cd = self.Cd\n",
        "        gap_distance_in_wavelengths = 1.\n",
        "\n",
        "        # get initial condition\n",
        "        if initial_condition is None:\n",
        "            if initial_condition_fn is not None:\n",
        "                initial_condition = initial_condition_fn()\n",
        "            else:\n",
        "                initial_condition = InitialCondition()\n",
        "\n",
        "        self.A1 = initial_condition.A1\n",
        "        self.f1 = initial_condition.f1\n",
        "        self.A2 = initial_condition.A2\n",
        "        self.f2 = initial_condition.f2\n",
        "        self.goal = initial_condition.goal\n",
        "\n",
        "        u2_initial = initial_condition.u2\n",
        "\n",
        "        self.distance = initial_condition.distance\n",
        "        self.distance_from_goal = numpy.abs(self.distance - self.goal)\n",
        "        self.flow_agreement = initial_condition.flow_agreement\n",
        "        self.flap = numpy.asarray([-initial_condition.distance, u2_initial])\n",
        "        self.tt = 0\n",
        "        self.previous_v_flow = 0\n",
        "        self.flow_agreement_history = []\n",
        "        self.previous_x2 = -initial_condition.distance\n",
        "        self.flow_agreement = initial_condition.flow_agreement\n",
        "        self.avg_flow_agreement = 0\n",
        "        self.u2 = .1\n",
        "\n",
        "        return self._get_obs()\n",
        "\n",
        "    def render(self, mode='rgb_array'):\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.scatter(self.x1, self.y1, label='leader')\n",
        "        ax.scatter([self.flap[0]], [self.flap[1]], label='follower')\n",
        "        ax.set_xlim([self.x1 - 50, self.x1 + 5])\n",
        "        ax.set_ylim([-2*self.A1, 2*self.A1])\n",
        "        ax.legend()\n",
        "        fig.canvas.draw()\n",
        "        fig.canvas.tostring_rgb()\n",
        "        frame = numpy.frombuffer(fig.canvas.tostring_rgb(), dtype=numpy.uint8)\n",
        "        frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "        plt.close(fig)\n",
        "        return frame\n",
        "\n",
        "    def close(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "Kp-2bJ7mSifa"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Memory(object):\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "    def clear_memory(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.is_terminals[:]\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.affine = nn.Linear(state_dim, n_latent_var)\n",
        "\n",
        "        # actor\n",
        "        self.action_layer = nn.Sequential(\n",
        "                nn.Linear(state_dim, n_latent_var),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(n_latent_var, n_latent_var),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(n_latent_var, action_dim),\n",
        "                nn.Softmax(dim=-1)\n",
        "                )\n",
        "\n",
        "        # critic\n",
        "        self.value_layer = nn.Sequential(\n",
        "                nn.Linear(state_dim, n_latent_var),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(n_latent_var, n_latent_var),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(n_latent_var, 1)\n",
        "                )\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def act(self, state, memory):\n",
        "        state = torch.from_numpy(state).float().to(device)\n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "\n",
        "        memory.states.append(state)\n",
        "        memory.actions.append(action)\n",
        "        memory.logprobs.append(dist.log_prob(action))\n",
        "\n",
        "        return action.item()\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "\n",
        "        state_value = self.value_layer(state)\n",
        "\n",
        "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
        "\n",
        "class PPO(object):\n",
        "    def __init__(self, state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip):\n",
        "        self.lr = lr\n",
        "        self.betas = betas\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "    def update(self, memory):\n",
        "        # Monte Carlo estimate of state rewards:\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        # Normalizing the rewards:\n",
        "        rewards = torch.tensor(rewards).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
        "        rewards = rewards.type(torch.float32)\n",
        "\n",
        "        # convert list to tensor\n",
        "        old_states = torch.stack(memory.states).to(device).detach()\n",
        "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
        "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
        "\n",
        "        # Optimize policy for K epochs:\n",
        "        for _ in range(self.K_epochs):\n",
        "            # Evaluating old actions and values :\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # Ensure state_values are float32\n",
        "            state_values = state_values.type(torch.float32)\n",
        "\n",
        "            # Finding the ratio (pi_theta / pi_theta__old):\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Finding Surrogate Loss:\n",
        "            advantages = rewards - state_values.detach()\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
        "\n",
        "            # take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Copy new weights into old policy:\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, **env_args):\n",
        "        ############## Hyperparameters ##############\n",
        "        self.env_name = \"Flappers\"\n",
        "        self.env = SwimmerEnv(**env_args)\n",
        "        self.state_dim = self.env.observation_space.shape[0]\n",
        "        self.action_dim = 3\n",
        "        self.render = False\n",
        "        self.solved_reward = 500         # stop training if avg_reward > solved_reward\n",
        "        self.log_interval = 20           # print avg reward in the interval\n",
        "        self.max_episodes = 1000        # max training episodes\n",
        "        self.max_timesteps = 500       # max timesteps in one episode\n",
        "        self.n_latent_var = 64           # number of variables in hidden layer\n",
        "        self.update_timestep = 200    # update policy every n timesteps\n",
        "        self.lr = 0.002\n",
        "        self.betas = (0.9, 0.999)\n",
        "        self.gamma = 0.99                # discount factor\n",
        "        self.K_epochs = 4               # update policy for K epochs\n",
        "        self.eps_clip = 0.2              # clip parameter for PPO\n",
        "        self.random_seed = 4\n",
        "        #############################################\n",
        "\n",
        "        if self.random_seed:\n",
        "            torch.manual_seed(self.random_seed)\n",
        "            self.env.seed(self.random_seed)\n",
        "\n",
        "        self.memory = Memory()\n",
        "        self.ppo = PPO(self.state_dim, self.action_dim, self.n_latent_var, self.lr, self.betas, self.gamma, self.K_epochs, self.eps_clip)\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        self.ppo.policy.load_state_dict(torch.load(model_path))\n",
        "        self.ppo.policy_old.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    def train(self, initial_condition=None, initial_condition_fn=None, episodes=None):\n",
        "\n",
        "        # logging variables\n",
        "        running_reward = 0\n",
        "        avg_length = 0\n",
        "        timestep = 0\n",
        "        history = []\n",
        "\n",
        "        # header\n",
        "        with open(f\"log_PPO_{self.env_name}_ACT_{self.env.action}_OBS_{self.env.observations}_REW_{self.env.rewards}.csv\", 'a') as f:\n",
        "                    f.write(f'Episode,avg length,reward\\n')\n",
        "        # training loop\n",
        "        if episodes is None:\n",
        "            episodes = self.max_episodes\n",
        "        for i_episode in range(1, episodes+1):\n",
        "            state = self.env.reset(initial_condition=initial_condition, initial_condition_fn=initial_condition_fn)\n",
        "            for t in range(self.max_timesteps):\n",
        "                timestep += 1\n",
        "\n",
        "                # Running policy_old:\n",
        "                action = self.ppo.policy_old.act(state, self.memory)\n",
        "                state, reward, done, info = self.env.step(action)\n",
        "\n",
        "                # Saving reward and is_terminal:\n",
        "                self.memory.rewards.append(reward)\n",
        "                self.memory.is_terminals.append(done)\n",
        "\n",
        "                # update if its time\n",
        "                if timestep % self.update_timestep == 0:\n",
        "                    self.ppo.update(self.memory)\n",
        "                    self.memory.clear_memory()\n",
        "                    timestep = 0\n",
        "\n",
        "                running_reward += reward\n",
        "                if self.render:\n",
        "                    self.env.render()\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "                history.append(info)\n",
        "\n",
        "            avg_length += t\n",
        "\n",
        "            filename = f\"./PPO_{self.env_name}_{i_episode}_ACT_{self.env.action}_OBS_{self.env.observations}_REW_{self.env.rewards}.pth\"\n",
        "            # stop training if avg_reward > solved_reward\n",
        "            if running_reward > (self.log_interval*self.solved_reward):\n",
        "                print(\"########## Solved! ##########\")\n",
        "                torch.save(self.ppo.policy.state_dict(), filename)\n",
        "                break\n",
        "            # save every 100 episodes\n",
        "            if i_episode % 100 == 0:\n",
        "                    torch.save(self.ppo.policy.state_dict(), filename)\n",
        "            # logging\n",
        "            if i_episode % self.log_interval == 0:\n",
        "                avg_length = int(avg_length/self.log_interval)\n",
        "                running_reward = ((running_reward/self.log_interval))\n",
        "\n",
        "                print('Episode {} \\t avg length: {} \\t reward: {}'.format(i_episode, avg_length, running_reward))\n",
        "\n",
        "                # Save message on csv file\n",
        "                with open(f\"log_PPO_{self.env_name}_ACT_{self.env.action}_OBS_{self.env.observations}_REW_{self.env.rewards}.csv\", 'a') as f:\n",
        "                    f.write(f'{i_episode},{avg_length},{running_reward}\\n')\n",
        "                running_reward = 0\n",
        "                avg_length = 0\n",
        "\n",
        "        return history\n",
        "\n",
        "    def plot_policy(self, fa_range=None, f2_range=None, a2_range=None, d_range=None, u_range=None):\n",
        "            action_choices = []\n",
        "\n",
        "            if len(self.env.observations) == 0:\n",
        "                raise ValueError(\"No observations provided\")\n",
        "            elif len(self.env.observations) == 1:\n",
        "                pair = (self.env.observations[0], self.env.observations[0])\n",
        "            elif len(self.env.observations) == 2:\n",
        "                pair = (self.env.observations[0], self.env.observations[1])\n",
        "            else:\n",
        "                raise ValueError(\"Too many observations provided\")\n",
        "\n",
        "            ranges = {\n",
        "                'flow agreement': fa_range,\n",
        "                'distance': d_range,\n",
        "                'f2': f2_range,\n",
        "                'A2': a2_range,\n",
        "                'u2': u_range\n",
        "            }\n",
        "            range1 = ranges[pair[0]]\n",
        "            range2 = ranges[pair[1]]\n",
        "\n",
        "            for x1 in range1:\n",
        "                for x2 in range2:\n",
        "                    state = numpy.array([x1, x2], dtype=numpy.float32)\n",
        "                    state_tensor = torch.from_numpy(state).float().to(device)\n",
        "                    with torch.no_grad():\n",
        "                        action_probs = self.ppo.policy.action_layer(state_tensor).cpu().numpy()\n",
        "                    action = numpy.argmax(action_probs)\n",
        "                    action_choices.append((x1, x2, action))\n",
        "\n",
        "            action_choices = numpy.array(action_choices)\n",
        "            print(action_choices.shape)\n",
        "\n",
        "            plt.figure(figsize=(12, 6))\n",
        "\n",
        "            scatter = plt.scatter(action_choices[:, 0], action_choices[:, 1], c=action_choices[:, 2], cmap='viridis', marker='o')\n",
        "            handles = [plt.Line2D([], [], color=scatter.cmap(scatter.norm(i)), marker='o', linestyle='', markersize=5, label=label)\n",
        "                      for i, label in enumerate(['Reduce', 'Mantain', 'Increase'])]\n",
        "            plt.legend(handles=handles, title='Chosen Action')\n",
        "            plt.title(f'Chosen Action Based on State ({pair[0]} and {pair[1]})')\n",
        "            plt.xlabel(pair[0])\n",
        "            plt.ylabel(pair[1])\n",
        "            plt.xticks(numpy.linspace(min(range1), max(range1), 11))\n",
        "            plt.yticks(numpy.linspace(min(range2), max(range2), 11))\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "    def test_initial_conditions(self, initial_conditions):\n",
        "\n",
        "        max_timesteps = 500\n",
        "\n",
        "        # plt.figure(figsize=(10, 6))\n",
        "        fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1, figsize=(10, 12))\n",
        "\n",
        "        for ic in initial_conditions:\n",
        "            obs = self.env.reset(initial_condition=ic)\n",
        "            schooling_numbers = []\n",
        "            flow_agreements = []\n",
        "            avg_flow_agreements = []\n",
        "            v_gradients = []\n",
        "            v2 = []\n",
        "            v2_tail = []\n",
        "            for t in range(max_timesteps):\n",
        "                action = self.ppo.policy.act(obs, Memory())\n",
        "                obs, reward, done, info = self.env.step(action)\n",
        "                schooling_number = (info['distance']-SwimmerEnv.c)/info['u1']/info['freq1']\n",
        "                schooling_numbers.append(schooling_number)\n",
        "                flow_agreements.append(info['flow_agreement'])\n",
        "                avg_flow_agreements.append(info['avg_flow_agreement'])\n",
        "                v_gradients.append(info['v_gradient'])\n",
        "                v2.append(info['v_flow'])\n",
        "                v2_tail.append(info['v_flow_head'])\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            # plt.plot(distances, label=f\"Initial Distance: {ic.distance}\")\n",
        "            ax1.plot(schooling_numbers, label=f\"Initial Distance: {ic.distance}\")\n",
        "\n",
        "            ax2.plot(flow_agreements, label=f\"Initial Distance: {ic.distance}\")\n",
        "\n",
        "            ax3.plot(avg_flow_agreements, label=f\"Initial Distance: {ic.distance}\")\n",
        "\n",
        "            ax4.plot(v_gradients, label=f\"Initial Distance: {ic.distance}\")\\\n",
        "\n",
        "            ax5.plot(v2, label=f\"Initial Distance: {ic.distance}\")\n",
        "            ax5.plot(v2_tail, label=f\"Initial Distance: {ic.distance}\")\n",
        "\n",
        "        ax1.set_xlabel('Timesteps (0.1 s)')\n",
        "        ax1.set_ylabel('Schooling Number')\n",
        "        ax1.set_title('Distance Between Leader and Follower for Different Initial Conditions')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        ax2.set_xlabel('Timesteps (0.1 s)')\n",
        "        ax2.set_ylabel('Flow Agreement')\n",
        "        # ax2.set_xlim([0, 100])\n",
        "        ax2.set_title('Flow Agreement for Different Initial Conditions')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        ax3.set_xlabel('Timesteps (0.1 s)')\n",
        "        ax3.set_ylabel('Average Flow Agreement')\n",
        "        # ax3.set_xlim([0, 100])\n",
        "        ax3.set_title('Average Flow Agreement for Different Initial Conditions')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True)\n",
        "\n",
        "        ax4.set_xlabel('Timesteps (0.1 s)')\n",
        "        ax4.set_ylabel('Velocity Gradient')\n",
        "        # ax4.set_xlim([0,100])\n",
        "        std_vel_gradient = numpy.std(v_gradients)\n",
        "        ax4.set_ylim([-3*std_vel_gradient,3*std_vel_gradient])\n",
        "        ax4.set_title('Velocity Gradient for Different Initial Conditions')\n",
        "        ax4.legend()\n",
        "        ax4.grid(True)\n",
        "\n",
        "        ax5.set_xlabel('Timesteps (0.1 s)')\n",
        "        ax5.set_ylabel('Velocity')\n",
        "        # ax5.set_xlim([0,100])\n",
        "        ax5.set_title('Velocity for Different Initial Conditions')\n",
        "        ax5.legend()\n",
        "        ax5.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def naca_airfoil(self, code, num_points=100):\n",
        "        \"\"\"Generates the coordinates of a NACA 4-digit airfoil.\"\"\"\n",
        "        m = float(code[0]) / 100.0  # Maximum camber\n",
        "        p = float(code[1]) / 10.0  # Location of maximum camber\n",
        "        t = float(code[2:]) / 100.0  # Maximum thickness\n",
        "\n",
        "        x = numpy.linspace(0, 1, num_points)\n",
        "        yt = 5 * t * (0.2969 * numpy.sqrt(x) - 0.1260 * x - 0.3516 * x**2 + 0.2843 * x**3 - 0.1015 * x**4)\n",
        "\n",
        "        if m == 0 and p == 0:\n",
        "            yc = numpy.zeros_like(x)\n",
        "            theta = numpy.zeros_like(x)\n",
        "        else:\n",
        "            yc = numpy.where(x <= p,\n",
        "                            m / p**2 * (2 * p * x - x**2),\n",
        "                            m / (1 - p)**2 * ((1 - 2 * p) + 2 * p * x - x**2))\n",
        "            theta = numpy.arctan(numpy.gradient(yc, x))\n",
        "\n",
        "        xu = x - yt * numpy.sin(theta)\n",
        "        yu = yc + yt * numpy.cos(theta)\n",
        "        xl = x + yt * numpy.sin(theta)\n",
        "        yl = yc - yt * numpy.cos(theta)\n",
        "\n",
        "        # Close the path by adding the first point to the end\n",
        "        x_coords = -numpy.concatenate([xu, xl[::-1]])\n",
        "        y_coords = numpy.concatenate([yu, yl[::-1]])\n",
        "\n",
        "        return mpath.Path(numpy.column_stack([x_coords, y_coords]))\n",
        "\n",
        "    def animate(self, i):\n",
        "        font_prop = fm.FontProperties(size=35)\n",
        "        self.ax.clear()\n",
        "        x_leader = self.leader_positions[i][0]\n",
        "        y_leader = self.leader_positions[i][1]\n",
        "        x_follower = self.follower_positions[i][0]\n",
        "        y_follower = self.follower_positions[i][1]\n",
        "        self.ax.set_xlim([x_leader - 50, x_leader + 5])\n",
        "        self.ax.set_ylim([-1.5 * self.env.A1, 1.5 * self.env.A1])\n",
        "        self.ax.scatter(x_leader, y_leader, label='leader', color='black', marker=self.fish_marker)\n",
        "        self.ax.scatter(x_follower, y_follower, label='follower', color='black', marker=self.fish_marker)\n",
        "        self.ax.grid(True, axis='x', color='grey')\n",
        "        self.ax.set_xlabel('X Position (cm)', fontproperties = font_prop)\n",
        "        self.ax.set_ylabel('Y Position (cm)', fontproperties = font_prop)\n",
        "        self.ax.tick_params(axis='both', which='major', labelsize=30)\n",
        "\n",
        "        # Display wake\n",
        "        for j, wake_pos in enumerate(self.leader_positions[:i]):\n",
        "            alpha = 1.0 - (i-j) / len(self.leader_positions)\n",
        "            wake = self.v_flow_history[j]\n",
        "            marker_size = 5 + 5*abs(wake)\n",
        "            self.ax.scatter(wake_pos[0], wake_pos[1], color='blue', alpha=alpha, s=marker_size, marker='o')\n",
        "\n",
        "        return self.ax\n",
        "\n",
        "    def create_video(self, ic=InitialCondition(distance=30, f2=1.), time=10):\n",
        "        airfoil_path = self.naca_airfoil(\"0016\")\n",
        "        state = self.env.reset(ic)\n",
        "\n",
        "        self.fish_marker = MarkerStyle(airfoil_path, transform=mpl.transforms.Affine2D().scale(16))\n",
        "\n",
        "        self.leader_positions = []\n",
        "        self.follower_positions = []\n",
        "        frame_rate = 12\n",
        "        runtime = time # seconds\n",
        "\n",
        "        self.v_flow_history = []\n",
        "\n",
        "        for _ in range(frame_rate*runtime):\n",
        "            action = self.ppo.policy.act(state, Memory())\n",
        "            state, reward, done, info = self.env.step(action)\n",
        "\n",
        "            self.leader_positions.append((info['x1'], info['y1']))\n",
        "            self.follower_positions.append((info['x2'], info['y2']))\n",
        "            self.v_flow_history.append(info['v_flow'])\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        fig, self.ax = plt.subplots(figsize=(21, 9))\n",
        "        ani = animation.FuncAnimation(fig, self.animate, frames=len(self.leader_positions), interval=frame_rate, blit=False)\n",
        "\n",
        "        # To save the animation as a video file:\n",
        "        ani.save('swimmer_animation.mp4', writer='ffmpeg', fps=frame_rate)\n",
        "\n",
        "        # Plot distances\n",
        "        distances = [numpy.linalg.norm(numpy.array(leader_pos) - numpy.array(follower_pos))\n",
        "                    for leader_pos, follower_pos in zip(self.leader_positions, self.follower_positions)]\n",
        "        times = numpy.arange(len(distances)) * (1.0 / frame_rate)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(times, distances)\n",
        "        plt.xlabel('Time (seconds)')\n",
        "        plt.ylabel('Distance')\n",
        "        plt.title('Distance between Leader and Follower vs. Time')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        return;"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL CONTROL**"
      ],
      "metadata": {
        "id": "4WJ5ECTGRw75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rH_orrnYMXT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/victorbuenog/Flapper.git"
      ],
      "metadata": {
        "id": "gC5gRtAYMvBD",
        "outputId": "99f9a21e-20a0-4c88-94bd-25bba34781ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Flapper'...\n",
            "remote: Enumerating objects: 168, done.\u001b[K\n",
            "remote: Counting objects: 100% (168/168), done.\u001b[K\n",
            "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
            "remote: Total 168 (delta 27), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (168/168), 9.98 MiB | 14.07 MiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "trainer = Trainer(\n",
        "    action='f2',  # None, 'f2', or 'A2'\n",
        "    observations=['f2', 'distance'],  # may contain: 'distance', 'f2', 'A2', 'flow agreement', 'avg flow agreement'\n",
        "    rewards=['distance'],  # may contain: 'distance', 'flow agreement'\n",
        ")\n",
        "\n",
        "# train from scratch\n",
        "icfn = lambda: InitialCondition().random(['A2','distance'])\n",
        "trainer.train(InitialCondition(f2=numpy.random.uniform(0.5,1.5), distance=numpy.random.uniform(10,30)), episodes=1000)\n",
        "\n",
        "def get_policy(model, episodes=1000):\n",
        "    pol_name = f\"PPO_{model.env_name}_{episodes}_ACT_{model.env.action}_OBS_{model.env.observations}_REW_{model.env.rewards}.pth\"\n",
        "    return pol_name\n",
        "\n",
        "# load from pth\n",
        "# model_path = f\"Flapper/Policies/{get_policy(trainer, episodes=1000)}\"\n",
        "# print(f\"Loading policy: {model_path}\")\n",
        "# trainer.load_model(model_path)"
      ],
      "metadata": {
        "id": "CDLcqR4R51SN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "outputId": "95f22103-e248-4f45-9fdf-8cc2e0a81bbb",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20 \t avg length: 120 \t reward: -135.7306636638813\n",
            "Episode 40 \t avg length: 166 \t reward: -124.636586209288\n",
            "Episode 60 \t avg length: 184 \t reward: -114.7623922952987\n",
            "Episode 80 \t avg length: 125 \t reward: -119.90076163921235\n",
            "Episode 100 \t avg length: 155 \t reward: -116.2575374265477\n",
            "Episode 120 \t avg length: 174 \t reward: -136.71849179088912\n",
            "Episode 140 \t avg length: 146 \t reward: -120.18041433171068\n",
            "Episode 160 \t avg length: 147 \t reward: -129.90420411116506\n",
            "Episode 180 \t avg length: 192 \t reward: -169.64809486712804\n",
            "Episode 200 \t avg length: 83 \t reward: -121.30999463329056\n",
            "Episode 220 \t avg length: 53 \t reward: -121.10073461850584\n",
            "Episode 240 \t avg length: 82 \t reward: -121.35401134249055\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7a98c24ef8e4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# train from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0micfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInitialCondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInitialCondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-79a0dab70c76>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, initial_condition, initial_condition_fn, episodes)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;31m# Running policy_old:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_old\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# Saving reward and is_terminal:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-d16e5c385bf9>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mv_flow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_delay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_delay\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_delay_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mv_flow_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_delay_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_delay_head\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mv_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv_flow\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv_flow_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.test_initial_conditions(InitialCondition(distance=dd, f2=1.1) for dd in range(20, 41, 5))"
      ],
      "metadata": {
        "id": "h8dSJB6F76DP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the policy\n",
        "fa_range = numpy.linspace(-10, 10, 100)\n",
        "f2_range = numpy.linspace(0.5, 1.5, 100)\n",
        "a2_range = numpy.linspace(0, 3, 100)\n",
        "d_range = numpy.linspace(0, 3, 100)\n",
        "v_range = numpy.linspace(-10, 10, 100)\n",
        "trainer.plot_policy(\n",
        "    fa_range=fa_range,\n",
        "    f2_range=f2_range,\n",
        "    a2_range=a2_range,\n",
        "    d_range=d_range,\n",
        "    u_range=v_range\n",
        ")"
      ],
      "metadata": {
        "id": "ksMYd2CQ5zeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.create_video(time=20)"
      ],
      "metadata": {
        "id": "x4_YXH-xnyX9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}