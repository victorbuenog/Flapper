{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorbuenog/Flapper/blob/test/Flappertorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "import numpy\n",
        "import scipy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.path as mpath\n",
        "from matplotlib.markers import MarkerStyle\n",
        "import matplotlib.font_manager as fm\n",
        "plt.ioff()\n",
        "\n",
        "import gym\n",
        "from gym import wrappers\n",
        "from gym import spaces\n",
        "\n",
        "import imageio"
      ],
      "metadata": {
        "id": "V1tlyuUFgqtI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QuAwnp9LVMnk"
      },
      "outputs": [],
      "source": [
        "class InitialCondition(object):\n",
        "\n",
        "    def __init__(self, distance=None, f2=None, A2=None, goal=None):\n",
        "        self.distance = distance if distance is not None else 21.5\n",
        "        self.A1 = 2.0\n",
        "        self.f1 = 1.0\n",
        "        self.A2 = 2.0 if A2 is None else A2\n",
        "        self.f2 = 1.0 if f2 is None else f2\n",
        "        self.goal = goal if goal is not None else 21.5\n",
        "\n",
        "        self.u2 = numpy.pi * self.A2 * self.f2 * numpy.sqrt(2 * SwimmerEnv.Ct / SwimmerEnv.Cd)\n",
        "        self.v2 = -self.A2*(2 * numpy.pi * self.f2)\n",
        "        self.t_delay = -self.distance/self.u2\n",
        "        self.v_flow = self.A1*self.f1*numpy.cos(2*numpy.pi*self.f1*(-self.t_delay))*numpy.exp(-self.t_delay/SwimmerEnv.T)\n",
        "        self.flow_agreement = self.v2 * self.v_flow\n",
        "\n",
        "    def random(self, randomize_fields=[]):\n",
        "        if 'distance' in randomize_fields:\n",
        "            self.distance = random.uniform(10,30)\n",
        "        if 'f2' in randomize_fields:\n",
        "            self.f2 = random.uniform(0.5, 1.5)\n",
        "        if 'A2' in randomize_fields:\n",
        "            self.A2 = random.uniform(.5, 3.0)\n",
        "        if 'v2' in randomize_fields:\n",
        "            self.v2 = random.uniform(-1.0, 1.0)\n",
        "        return self\n",
        "\n",
        "\n",
        "class SwimmerEnv(gym.Env):\n",
        "\n",
        "    s = 15.\n",
        "    c = 4.\n",
        "    As = s * c\n",
        "    T = 5\n",
        "    m = 80.\n",
        "    Ct = .96\n",
        "    Cd = .25\n",
        "    rho = 1.\n",
        "    dt = 0.1\n",
        "\n",
        "    def __init__(self, rewards=[]):\n",
        "        super(SwimmerEnv, self).__init__()\n",
        "\n",
        "        self.action_space = spaces.Discrete(5) # Increase, decrease, or maintain value for f2 and A2\n",
        "        self.n_history = 50 # observation timesteps\n",
        "\n",
        "        # Observations: f2, A2, distance, v2-v_flow, u2\n",
        "        obs_low = [-numpy.inf for _ in range(5*self.n_history)]\n",
        "        obs_high = [numpy.inf for _ in range(5*self.n_history)]\n",
        "\n",
        "        self.observation_space = spaces.Box(low=numpy.array(obs_low), high=numpy.array(obs_high), dtype=numpy.float32)\n",
        "        self.rewards = rewards\n",
        "\n",
        "        self.flap = None\n",
        "        self.t_bound = 500.\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def _shoot(self, A1, A2, f1, f2, vec_initial, t_start=0., t_bound=5000., method='RK45'):\n",
        "        rho = self.rho\n",
        "        As = self.As\n",
        "        T = self.T\n",
        "        m = self.m\n",
        "        Ct = self.Ct\n",
        "        Cd = self.Cd\n",
        "        s = self.s\n",
        "        c = self.c\n",
        "\n",
        "        def fun(t, vec):\n",
        "            x2, u2 = vec\n",
        "            u1 = numpy.pi * A1 * f1 * numpy.sqrt(2 * Ct / Cd)  # Leader velocity (constant)\n",
        "            dt = -x2 / u1 + t\n",
        "            self.Ft2 = 2*rho*As*Ct*numpy.pi**2*((A2*f2*numpy.cos(2*numpy.pi*f2*t)-A1*f1*numpy.cos(2*numpy.pi*f1*(t-dt))*numpy.exp(-dt/T)))**2\n",
        "            self.Fd2 = rho*As*Cd*u2**2/2\n",
        "            dy_dt = (u2, (self.Ft2 - self.Fd2)/m)\n",
        "            return numpy.asarray(dy_dt)\n",
        "        # events = [lambda t, y: y[0] - y[4] - 0.00001]\n",
        "        events = []\n",
        "        for ee in events: setattr(ee, 'terminal', True)\n",
        "        solver = scipy.integrate.solve_ivp(fun, (t_start, t_bound), vec_initial, method=method, events=events,\n",
        "                                            rtol=1e-4, atol=1e-7, max_step=.03, first_step=.001, dense_output=True)\n",
        "        u1 = numpy.pi * A1 * f1 * numpy.sqrt(2 * Ct / Cd)  # Leader velocity (constant)\n",
        "        x1 = u1 * solver.t[-1]  # Leader x-coord (calculated)\n",
        "        y1 = A1 * numpy.sin(2 * numpy.pi * f1 * solver.t[-1])  # Leader y-coord (calculated)\n",
        "        v1 = -A1*(2 * numpy.pi * f1)*numpy.cos(2 * numpy.pi * f1 * solver.t[-1])\n",
        "        y2 = A2 * numpy.sin(2 * numpy.pi * f2 * solver.t[-1])  # Leader y-coord (calculated)\n",
        "        v2 = -A2*(2 * numpy.pi * f2)*numpy.cos(2 * numpy.pi * f2 * solver.t[-1])\n",
        "        values = list(zip(solver.t, solver.y.T))\n",
        "\n",
        "        info = {\n",
        "            'x1': x1,\n",
        "            'y1': y1,\n",
        "            'u1': u1,\n",
        "            'v1': v1,\n",
        "            'x2': values[-1][1][0],\n",
        "            'y2': y2,\n",
        "            'u2': values[-1][1][1],\n",
        "            'v2': v2,\n",
        "        }\n",
        "        return solver, values, info\n",
        "\n",
        "\n",
        "    def _get_obs(self):\n",
        "        obs = [\n",
        "            self.f2,\n",
        "            self.A2,\n",
        "            self.distance,\n",
        "            self.v2 - self.v_flow,\n",
        "            self.avg_u2,\n",
        "        ]\n",
        "\n",
        "        return numpy.array(obs, dtype=numpy.float32)\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        Ct = self.Ct\n",
        "        Cd = self.Cd\n",
        "\n",
        "        match action:\n",
        "            case 0:\n",
        "                self.f2 = max(self.f2 - 0.1, 0.5)\n",
        "            case 1:\n",
        "                self.f2 = min(self.f2 + 0.1, 1.5)\n",
        "            case 2:\n",
        "                self.A2 = max(self.A2 - 0.1, 0.5)\n",
        "            case 3:\n",
        "                self.A2 = min(self.A2 + 0.1, 3.0)\n",
        "            case 4: # No change\n",
        "                self.A2 = self.A2\n",
        "                self.f2 = self.f2\n",
        "\n",
        "        t_bound_step = self.dt\n",
        "        solver, values, shoot_info = self._shoot(self.A1, self.A2, self.f1, self.f2, self.flap, t_start=self.tt, t_bound=self.tt+t_bound_step)\n",
        "        self.flap = values[-1][1]\n",
        "        self.tt += self.dt\n",
        "        self.u1 = numpy.pi * self.A1 * self.f1 * numpy.sqrt(2 * Ct / Cd)  # Leader velocity\n",
        "        self.x1 = self.u1 * self.tt  # Leader x-coord\n",
        "        self.y1 = self.A1 * numpy.sin(2 * numpy.pi * self.f1 * self.tt)  # Leader y-coord\n",
        "        self.y2 = self.A2 * numpy.sin(2 * numpy.pi * self.f2 * self.tt)  # Follower y-coord\n",
        "        self.v1 = -self.A1*(2 * numpy.pi * self.f1)*numpy.cos(2 * numpy.pi * self.f1 * self.tt)\n",
        "        self.t_delay = self.tt - self.flap[0]/self.u1\n",
        "        v_flow = self.A1*self.f1*numpy.cos(2*numpy.pi*self.f1*(self.tt-self.t_delay))*numpy.exp(-self.t_delay/self.T)\n",
        "        self.t_delay_head = self.tt - (self.flap[0]+self.c)/self.u1\n",
        "        v_flow_head = self.A1*self.f1*numpy.cos(2*numpy.pi*self.f1*(self.tt-self.t_delay_head))*numpy.exp(-self.t_delay_head/self.T)\n",
        "        v_gradient = (v_flow_head - v_flow)/self.c\n",
        "        self.u2 = self.flap[1]\n",
        "\n",
        "        alpha = -numpy.arctan2(shoot_info['v2']-v_flow, self.u2)\n",
        "        Cl = 2*numpy.pi*numpy.sin(alpha)\n",
        "        v2_dot = (shoot_info['v2'] - self.previous_v2)/self.dt\n",
        "        # L = Cl*self.rho*self.As*self.u2**2/2\n",
        "        # Fy = self.m*v2_dot - L*numpy.cos(-alpha)\n",
        "        L = 1*self.rho*self.As*(shoot_info['v2']-v_flow)**2/2\n",
        "        Fy = self.m*v2_dot - L\n",
        "        self.power = (Fy*shoot_info['v2']) / 1e5\n",
        "\n",
        "        self.distance = self.x1 - self.flap[0]-self.c  # Distance between leader and follower\n",
        "        done = False\n",
        "        new_distance_from_goal = numpy.abs(self.distance - self.goal) # Distance\n",
        "        new_flow_agreement = shoot_info['v2'] * v_flow_head # Flow agreement\n",
        "        a_flow = (v_flow-self.previous_v_flow)/self.dt # Flow acceleration\n",
        "\n",
        "\n",
        "        # Calculate average flow agreement over one period\n",
        "        self.flow_agreement_history.append(new_flow_agreement)\n",
        "        if len(self.flow_agreement_history) > 10:\n",
        "            self.flow_agreement_history.pop(0)\n",
        "        new_avg_flow_agreement = numpy.mean(self.flow_agreement_history)\n",
        "\n",
        "        # Calculate average follower velocity over one period\n",
        "        self.u2_history.append(self.u2)\n",
        "        if len(self.u2_history) > 10:\n",
        "            self.u2_history.pop(0)\n",
        "        self.avg_u2 = numpy.mean(self.u2_history)\n",
        "\n",
        "        # Power average\n",
        "        self.power_history.append(self.power)\n",
        "        if len(self.power_history) > 10:\n",
        "            self.power_history.pop(0)\n",
        "        self.avg_power = numpy.mean(self.power_history)\n",
        "\n",
        "        # distance average\n",
        "        self.distance_history.append(self.distance)\n",
        "        if len(self.distance_history) > self.n_history:\n",
        "            self.distance_history.pop(0)\n",
        "        self.avg_distance = numpy.mean(self.distance_history)\n",
        "\n",
        "        # Reset reward\n",
        "        self.reward = 0.\n",
        "\n",
        "        # Penalize change in distance\n",
        "        self.reward -= 0.1*abs(self.avg_distance - self.distance)\n",
        "\n",
        "        # Penalize extreme behaviors\n",
        "        if self.distance < 0. or self.distance > 200:\n",
        "            self.reward -= 100\n",
        "            done = True\n",
        "\n",
        "        self.previous_power = self.avg_power\n",
        "        self.previous_v_flow = v_flow\n",
        "        self.distance_from_goal = new_distance_from_goal\n",
        "        self.flow_agreement = new_flow_agreement\n",
        "        self.avg_flow_agreement = new_avg_flow_agreement\n",
        "        self.last_reward = self.reward\n",
        "        self.previous_v2 = shoot_info['v2']\n",
        "\n",
        "        # Update observation history\n",
        "        self.obs_history.append(self._get_obs())\n",
        "        if len(self.obs_history) > self.n_history:\n",
        "            self.obs_history.pop(0)\n",
        "        history_obs = numpy.array(self.obs_history).flatten()\n",
        "\n",
        "        info = {\n",
        "            'distance': self.distance,\n",
        "            'action': action,\n",
        "            'reward': self.reward,\n",
        "            'done': done,\n",
        "            'f1': self.f1,\n",
        "            'f2': self.f2,\n",
        "            'A2': self.A2,\n",
        "            'distance_from_goal': self.distance_from_goal,\n",
        "            't': self.tt,\n",
        "            'v_flow': v_flow,\n",
        "            'v_flow_head': v_flow_head,\n",
        "            'a_flow': a_flow,\n",
        "            'flow_agreement': new_flow_agreement,\n",
        "            'avg_flow_agreement': new_avg_flow_agreement,\n",
        "            'u2': self.flap[1],\n",
        "            'avg_u2': self.avg_u2,\n",
        "            'v_gradient': v_gradient,\n",
        "            'avg_power': self.avg_power,\n",
        "        }\n",
        "        info.update(shoot_info)\n",
        "\n",
        "        return history_obs, self.reward, done, info\n",
        "\n",
        "    def reset(self, initial_condition=None, initial_condition_fn=None):\n",
        "        Ct = self.Ct\n",
        "        Cd = self.Cd\n",
        "        gap_distance_in_wavelengths = 1.\n",
        "\n",
        "        # get initial condition\n",
        "        if initial_condition is None:\n",
        "            if initial_condition_fn is not None:\n",
        "                initial_condition = initial_condition_fn()\n",
        "            else:\n",
        "                initial_condition = InitialCondition()\n",
        "\n",
        "        self.A1 = initial_condition.A1\n",
        "        self.f1 = initial_condition.f1\n",
        "        self.A2 = initial_condition.A2\n",
        "        self.f2 = initial_condition.f2\n",
        "        self.goal = initial_condition.goal\n",
        "\n",
        "        u2_initial = initial_condition.u2\n",
        "\n",
        "        self.distance = initial_condition.distance\n",
        "        self.distance_from_goal = numpy.abs(self.distance - self.goal)\n",
        "        self.flow_agreement = initial_condition.flow_agreement\n",
        "        self.flap = numpy.asarray([-initial_condition.distance, u2_initial])\n",
        "        self.tt = 0\n",
        "        self.previous_v_flow = 0\n",
        "        self.flow_agreement_history = []\n",
        "        self.u2_history = []\n",
        "        self.power_history = []\n",
        "        self.previous_x2 = -self.distance\n",
        "        self.flow_agreement = initial_condition.flow_agreement\n",
        "        self.avg_flow_agreement = 0\n",
        "        self.u2 = .1\n",
        "        self.avg_u2 = .1\n",
        "        self.avg_power = .1\n",
        "        self.previous_power = .1\n",
        "        self.previous_v2 = .1\n",
        "        self.v2 = initial_condition.v2\n",
        "        self.v_flow = 0\n",
        "        self.distance_history = []\n",
        "\n",
        "        self.obs_history = [self._get_obs() for _ in range(self.n_history)]\n",
        "\n",
        "        history_obs = numpy.array(self.obs_history).flatten()\n",
        "\n",
        "        return history_obs\n",
        "\n",
        "    def render(self, mode='rgb_array'):\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.scatter(self.x1, self.y1, label='leader')\n",
        "        ax.scatter([self.flap[0]], [self.flap[1]], label='follower')\n",
        "        ax.set_xlim([self.x1 - 50, self.x1 + 5])\n",
        "        ax.set_ylim([-2*self.A1, 2*self.A1])\n",
        "        ax.legend()\n",
        "        fig.canvas.draw()\n",
        "        fig.canvas.tostring_rgb()\n",
        "        frame = numpy.frombuffer(fig.canvas.tostring_rgb(), dtype=numpy.uint8)\n",
        "        frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "        plt.close(fig)\n",
        "        return frame\n",
        "\n",
        "    def close(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "id": "Kp-2bJ7mSifa"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Memory(object):\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "    def clear_memory(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.is_terminals[:]\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        # actor\n",
        "        self.action_layer = nn.Sequential(\n",
        "                nn.Linear(state_dim, n_latent_var),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(n_latent_var, n_latent_var),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(n_latent_var, action_dim),\n",
        "                nn.Softmax(dim=-1)\n",
        "                )\n",
        "\n",
        "        # critic\n",
        "        self.value_layer = nn.Sequential(\n",
        "                nn.Linear(state_dim, n_latent_var),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(n_latent_var, n_latent_var),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(n_latent_var, 1)\n",
        "                )\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def act(self, state, memory):\n",
        "        state = torch.from_numpy(state).float().to(device)\n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "\n",
        "        memory.states.append(state)\n",
        "        memory.actions.append(action)\n",
        "        memory.logprobs.append(dist.log_prob(action))\n",
        "\n",
        "        return action.item()\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "\n",
        "        state_value = self.value_layer(state)\n",
        "\n",
        "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
        "\n",
        "class PPO(object):\n",
        "    def __init__(self, state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip):\n",
        "        self.lr = lr\n",
        "        self.betas = betas\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "    def update(self, memory):\n",
        "        # Monte Carlo estimate of state rewards:\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        # Normalizing the rewards:\n",
        "        rewards = torch.tensor(rewards).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
        "        rewards = rewards.type(torch.float32)\n",
        "\n",
        "        # convert list to tensor\n",
        "        old_states = torch.stack(memory.states).to(device).detach()\n",
        "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
        "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
        "\n",
        "        # Optimize policy for K epochs:\n",
        "        for _ in range(self.K_epochs):\n",
        "            # Evaluating old actions and values :\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # Ensure state_values are float32\n",
        "            state_values = state_values.type(torch.float32)\n",
        "\n",
        "            # Finding the ratio (pi_theta / pi_theta__old):\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Finding Surrogate Loss:\n",
        "            advantages = rewards - state_values.detach()\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
        "\n",
        "            # take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Copy new weights into old policy:\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, **env_args):\n",
        "        ############## Hyperparameters ##############\n",
        "        self.env_name = \"Flappers\"\n",
        "        self.env = SwimmerEnv(**env_args)\n",
        "        self.state_dim = self.env.observation_space.shape[0]\n",
        "        self.action_dim = 5\n",
        "        self.render = False\n",
        "        self.solved_reward = 500         # stop training if avg_reward > solved_reward\n",
        "        self.log_interval = 20           # print avg reward in the interval\n",
        "        self.max_episodes = 1000        # max training episodes\n",
        "        self.max_timesteps = 500       # max timesteps in one episode\n",
        "        self.n_latent_var = 64           # number of variables in hidden layer\n",
        "        self.update_timestep = 200    # update policy every n timesteps\n",
        "        self.lr = 0.002\n",
        "        self.betas = (0.9, 0.999)\n",
        "        self.gamma = 0.99                # discount factor\n",
        "        self.K_epochs = 4               # update policy for K epochs\n",
        "        self.eps_clip = 0.2              # clip parameter for PPO\n",
        "        self.random_seed = 4\n",
        "        #############################################\n",
        "\n",
        "        if self.random_seed:\n",
        "            torch.manual_seed(self.random_seed)\n",
        "            self.env.seed(self.random_seed)\n",
        "\n",
        "        self.memory = Memory()\n",
        "        self.ppo = PPO(self.state_dim, self.action_dim, self.n_latent_var, self.lr, self.betas, self.gamma, self.K_epochs, self.eps_clip)\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        self.ppo.policy.load_state_dict(torch.load(model_path))\n",
        "        self.ppo.policy_old.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    def get_policy(self, episodes=1000):\n",
        "        pol_name = f\"PPO_{self.env_name}_{episodes}.pth\"\n",
        "        return pol_name\n",
        "\n",
        "    def train(self, initial_condition=None, initial_condition_fn=None, episodes=None):\n",
        "\n",
        "        # logging variables\n",
        "        running_reward = 0\n",
        "        avg_length = 0\n",
        "        timestep = 0\n",
        "        history = []\n",
        "\n",
        "        # header\n",
        "        with open(f\"log_PPO_{self.env_name}.csv\", 'a') as f:\n",
        "                    f.write(f'Episode,avg length,reward\\n')\n",
        "        # training loop\n",
        "        if episodes is None:\n",
        "            episodes = self.max_episodes\n",
        "        for i_episode in range(1, episodes+1):\n",
        "            state = self.env.reset(initial_condition=initial_condition, initial_condition_fn=initial_condition_fn)\n",
        "            for t in range(self.max_timesteps):\n",
        "                timestep += 1\n",
        "\n",
        "                # Running policy_old:\n",
        "                action = self.ppo.policy_old.act(state, self.memory)\n",
        "                state, reward, done, info = self.env.step(action)\n",
        "\n",
        "                # Saving reward and is_terminal:\n",
        "                self.memory.rewards.append(reward)\n",
        "                self.memory.is_terminals.append(done)\n",
        "\n",
        "                # update if its time\n",
        "                if timestep % self.update_timestep == 0:\n",
        "                    self.ppo.update(self.memory)\n",
        "                    self.memory.clear_memory()\n",
        "                    timestep = 0\n",
        "\n",
        "                running_reward += reward\n",
        "                if self.render:\n",
        "                    self.env.render()\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "                history.append(info)\n",
        "\n",
        "            avg_length += t\n",
        "\n",
        "            filename = f\"./PPO_{self.env_name}_{i_episode}.pth\"\n",
        "\n",
        "            # save every 1000 episodes\n",
        "            if i_episode % 1000 == 0:\n",
        "                    torch.save(self.ppo.policy.state_dict(), filename)\n",
        "            # logging\n",
        "            if i_episode % self.log_interval == 0:\n",
        "                avg_length = int(avg_length/self.log_interval)\n",
        "                running_reward = ((running_reward/self.log_interval))\n",
        "\n",
        "                print('Episode {} \\t avg length: {} \\t reward: {}'.format(i_episode, avg_length, running_reward))\n",
        "\n",
        "                # Save message on csv file\n",
        "                with open(f\"log_PPO_{self.env_name}.csv\", 'a') as f:\n",
        "                    f.write(f'{i_episode},{avg_length},{running_reward}\\n')\n",
        "                running_reward = 0\n",
        "                avg_length = 0\n",
        "\n",
        "        return history;\n",
        "\n",
        "    def test_policy(self, initial_conditions):\n",
        "\n",
        "        max_timesteps = 500\n",
        "\n",
        "        fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1, figsize=(10, 20))\n",
        "\n",
        "        for ic in initial_conditions:\n",
        "            obs = self.env.reset(initial_condition=ic)\n",
        "            schooling_numbers = []\n",
        "            flow_agreements = []\n",
        "            avg_flow_agreements = []\n",
        "            v_gradients = []\n",
        "            u2 = []\n",
        "            action_values = []\n",
        "            p_values = []\n",
        "            for t in range(max_timesteps):\n",
        "                action = self.ppo.policy.act(obs, Memory())\n",
        "                obs, reward, done, info = self.env.step(action)\n",
        "                schooling_number = (info['distance']-SwimmerEnv.c)/info['u1']/info['f1']\n",
        "                schooling_numbers.append(schooling_number)\n",
        "                flow_agreements.append(info['flow_agreement'])\n",
        "                avg_flow_agreements.append(info['avg_flow_agreement'])\n",
        "                v_gradients.append(info['v_gradient'])\n",
        "                u2.append(info['avg_u2'])\n",
        "                p_values.append(info['avg_power'])\n",
        "                if self.env.action is not None:\n",
        "                    key = self.env.action\n",
        "                else:\n",
        "                    key = self.env.observations[0]\n",
        "                action_values.append(info[key])\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            # plt.plot(distances, label=f\"Initial Distance: {ic.distance}\")\n",
        "            ax1.plot(schooling_numbers, label=f\"Initial Distance: {ic.distance}\")\n",
        "\n",
        "            ax2.plot(p_values, label=f\"Initial Distance: {ic.distance}\")\n",
        "\n",
        "            ax3.plot(avg_flow_agreements, label=f\"Initial Distance: {ic.distance}\")\n",
        "\n",
        "            ax4.plot(v_gradients, label=f\"Initial Distance: {ic.distance}\")\n",
        "\n",
        "            ax5.plot(u2, label=f\"Initial Distance: {ic.distance}\")\n",
        "\n",
        "        ax1.set_xlabel('Timesteps (0.1 s)')\n",
        "        ax1.set_ylabel('Schooling Number')\n",
        "        ax1.set_title('Distance Between Leader and Follower for Different Initial Conditions')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        ax2.set_xlabel('Timesteps (0.1 s)')\n",
        "        ax2.set_ylabel('Average Power')\n",
        "        # ax2.set_xlim([0, 100])\n",
        "        ax2.set_title('Power for Different Initial Conditions')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        ax3.set_xlabel('Timesteps (0.1 s)')\n",
        "        ax3.set_ylabel('Average Flow Agreement')\n",
        "        # ax3.set_xlim([0, 100])\n",
        "        ax3.set_title('Average Flow Agreement for Different Initial Conditions')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True)\n",
        "\n",
        "        ax4.set_xlabel('Timesteps (0.1 s)')\n",
        "        ax4.set_ylabel('Velocity Gradient')\n",
        "        # ax4.set_xlim([0,100])\n",
        "        std_vel_gradient = numpy.std(v_gradients)\n",
        "        ax4.set_ylim([-3*std_vel_gradient,3*std_vel_gradient])\n",
        "        ax4.set_title('Velocity Gradient for Different Initial Conditions')\n",
        "        ax4.legend()\n",
        "        ax4.grid(True)\n",
        "\n",
        "        ax5.set_xlabel('Timesteps (0.1 s)')\n",
        "        ax5.set_ylabel('Horizontal Velocity')\n",
        "        # ax5.set_xlim([0,100])\n",
        "        ax5.set_title('Average Velocity for Different Initial Conditions')\n",
        "        ax5.legend()\n",
        "        ax5.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def naca_airfoil(self, code, num_points=100):\n",
        "        \"\"\"Generates the coordinates of a NACA 4-digit airfoil.\"\"\"\n",
        "        m = float(code[0]) / 100.0  # Maximum camber\n",
        "        p = float(code[1]) / 10.0  # Location of maximum camber\n",
        "        t = float(code[2:]) / 100.0  # Maximum thickness\n",
        "\n",
        "        x = numpy.linspace(0, 1, num_points)\n",
        "        yt = 5 * t * (0.2969 * numpy.sqrt(x) - 0.1260 * x - 0.3516 * x**2 + 0.2843 * x**3 - 0.1015 * x**4)\n",
        "\n",
        "        if m == 0 and p == 0:\n",
        "            yc = numpy.zeros_like(x)\n",
        "            theta = numpy.zeros_like(x)\n",
        "        else:\n",
        "            yc = numpy.where(x <= p,\n",
        "                            m / p**2 * (2 * p * x - x**2),\n",
        "                            m / (1 - p)**2 * ((1 - 2 * p) + 2 * p * x - x**2))\n",
        "            theta = numpy.arctan(numpy.gradient(yc, x))\n",
        "\n",
        "        xu = x - yt * numpy.sin(theta)\n",
        "        yu = yc + yt * numpy.cos(theta)\n",
        "        xl = x + yt * numpy.sin(theta)\n",
        "        yl = yc - yt * numpy.cos(theta)\n",
        "\n",
        "        # Close the path by adding the first point to the end\n",
        "        x_coords = -numpy.concatenate([xu, xl[::-1]])\n",
        "        y_coords = numpy.concatenate([yu, yl[::-1]])\n",
        "\n",
        "        return mpath.Path(numpy.column_stack([x_coords, y_coords]))\n",
        "\n",
        "    def animate(self, i):\n",
        "        font_prop = fm.FontProperties(size=35)\n",
        "        self.ax.clear()\n",
        "        x_leader = self.leader_positions[i][0]\n",
        "        y_leader = self.leader_positions[i][1]\n",
        "        x_follower = self.follower_positions[i][0]\n",
        "        y_follower = self.follower_positions[i][1]\n",
        "        self.ax.set_xlim([x_leader - 50, x_leader + 5])\n",
        "        self.ax.set_ylim([-1.5 * self.env.A1, 1.5 * self.env.A1])\n",
        "        self.ax.scatter(x_leader, y_leader, label='leader', color='black', marker=self.fish_marker)\n",
        "        self.ax.scatter(x_follower, y_follower, label='follower', color='black', marker=self.fish_marker)\n",
        "        self.ax.grid(True, axis='x', color='grey')\n",
        "        self.ax.set_xlabel('X Position (cm)', fontproperties = font_prop)\n",
        "        self.ax.set_ylabel('Y Position (cm)', fontproperties = font_prop)\n",
        "        self.ax.tick_params(axis='both', which='major', labelsize=30)\n",
        "\n",
        "        # Display wake\n",
        "        T = self.env.T\n",
        "        for j, wake_pos in enumerate(self.leader_positions[:i]):\n",
        "            t_delay = (i - j) / self.frame_rate\n",
        "            wake_amplitude_scale = numpy.exp(-t_delay / T)\n",
        "            self.ax.scatter(wake_pos[0], wake_pos[1]*wake_amplitude_scale, color='blue', s=5, marker='o')\n",
        "\n",
        "        return self.ax\n",
        "\n",
        "    def create_video(self, ic=InitialCondition(distance=30, f2=1.), time=10):\n",
        "        state = self.env.reset(ic)\n",
        "\n",
        "        airfoil_path = self.naca_airfoil(\"0017\")\n",
        "        self.fish_marker = MarkerStyle(airfoil_path, transform=mpl.transforms.Affine2D().scale(16))\n",
        "\n",
        "        self.leader_positions = []\n",
        "        self.follower_positions = []\n",
        "        self.frame_rate = 12\n",
        "        runtime = time # seconds\n",
        "\n",
        "        for _ in range(self.frame_rate*runtime):\n",
        "            action = self.ppo.policy.act(state, Memory())\n",
        "            state, reward, done, info = self.env.step(action)\n",
        "\n",
        "            self.leader_positions.append((info['x1'], info['y1']))\n",
        "            self.follower_positions.append((info['x2'], info['y2']))\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        fig, self.ax = plt.subplots(figsize=(21, 9))\n",
        "        ani = animation.FuncAnimation(fig, self.animate, frames=len(self.leader_positions), interval=self.frame_rate, blit=False)\n",
        "\n",
        "        # To save the animation as a video file:\n",
        "        ani.save('swimmer_animation.mp4', writer='ffmpeg', fps=self.frame_rate)\n",
        "\n",
        "        # Plot distances\n",
        "        distances = [numpy.linalg.norm(numpy.array(leader_pos) - numpy.array(follower_pos))\n",
        "                    for leader_pos, follower_pos in zip(self.leader_positions, self.follower_positions)]\n",
        "        times = numpy.arange(len(distances)) * (1.0 / self.frame_rate)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(times, distances)\n",
        "        plt.xlabel('Time (seconds)')\n",
        "        plt.ylabel('Distance')\n",
        "        plt.title('Distance between Leader and Follower vs. Time')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        return;"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL CONTROL**"
      ],
      "metadata": {
        "id": "4WJ5ECTGRw75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/victorbuenog/Flapper.git"
      ],
      "metadata": {
        "id": "gC5gRtAYMvBD",
        "outputId": "971e6edc-d2a3-4247-ccc9-a32431706ea6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Flapper' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full run\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# obs_action = ['f2','A2']\n",
        "# obs_flow = ['distance', 'flow agreement', 'avg flow agreement', 'velocity']\n",
        "# rewards = ['distance', 'flow agreement', 'avg flow agreement', 'velocity', 'flow acceleration']\n",
        "\n",
        "# for obs1 in obs_action:\n",
        "#   for obs2 in obs_flow:\n",
        "#     for rew in rewards:\n",
        "#       trainer = Trainer(\n",
        "#           action=obs1,  # None, 'f2', or 'A2'\n",
        "#           observations=[obs1, obs2],  # may contain: 'distance', 'f2', 'A2', 'flow\n",
        "#           rewards=[rew],  # may contain: 'distance', 'flow agreement'\n",
        "#       )\n",
        "#       icfn = lambda: InitialCondition().random([obs1,'distance'])\n",
        "#       trainer.train(initial_condition_fn=icfn, episodes=1000)\n",
        "\n",
        "#####################################################################################################################\n",
        "\n",
        "# Single Run\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "trainer = Trainer()\n",
        "\n",
        "# train from scratch\n",
        "icfn = lambda: InitialCondition().random(['f2','distance'])\n",
        "history  = trainer.train(initial_condition_fn=icfn, episodes=10000)\n",
        "\n",
        "# load from pth\n",
        "# model_path = f\"Flapper/Policies/{trainer.get_policy(episodes=1000)}\"\n",
        "# print(f\"Loading policy: {model_path}\")\n",
        "# trainer.load_model(model_path)"
      ],
      "metadata": {
        "id": "xbD6nUGvhm4z",
        "outputId": "109cda7f-3dbd-4273-d043-263595f0ef9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20 \t avg length: 194 \t reward: -399.2768618604339\n",
            "Episode 40 \t avg length: 187 \t reward: -460.0438604136901\n",
            "Episode 60 \t avg length: 195 \t reward: -460.09507508878096\n",
            "Episode 80 \t avg length: 151 \t reward: -330.59064376911977\n",
            "Episode 100 \t avg length: 170 \t reward: -264.0191979230678\n",
            "Episode 120 \t avg length: 299 \t reward: -394.2154331668801\n",
            "Episode 140 \t avg length: 167 \t reward: -237.30012848972555\n",
            "Episode 160 \t avg length: 228 \t reward: -497.178309633479\n",
            "Episode 180 \t avg length: 255 \t reward: -484.73956926273667\n",
            "Episode 200 \t avg length: 55 \t reward: -129.2054965792222\n",
            "Episode 220 \t avg length: 32 \t reward: -116.82163634325222\n",
            "Episode 240 \t avg length: 130 \t reward: -228.19493687847574\n",
            "Episode 260 \t avg length: 162 \t reward: -336.459340974276\n",
            "Episode 280 \t avg length: 107 \t reward: -252.70288847893735\n",
            "Episode 300 \t avg length: 36 \t reward: -131.1092181330261\n",
            "Episode 320 \t avg length: 133 \t reward: -273.899701921566\n",
            "Episode 340 \t avg length: 37 \t reward: -129.8746235918061\n",
            "Episode 360 \t avg length: 235 \t reward: -451.8564558917645\n",
            "Episode 380 \t avg length: 234 \t reward: -433.7716062375183\n",
            "Episode 400 \t avg length: 222 \t reward: -414.858743654888\n",
            "Episode 420 \t avg length: 215 \t reward: -431.2854724327899\n",
            "Episode 440 \t avg length: 224 \t reward: -432.8764646381683\n",
            "Episode 460 \t avg length: 203 \t reward: -443.66259535678546\n",
            "Episode 480 \t avg length: 21 \t reward: -110.69374546451465\n",
            "Episode 500 \t avg length: 71 \t reward: -192.97840820050914\n",
            "Episode 520 \t avg length: 191 \t reward: -464.0270766589466\n",
            "Episode 540 \t avg length: 176 \t reward: -455.889997785437\n",
            "Episode 560 \t avg length: 198 \t reward: -480.15237706544974\n",
            "Episode 580 \t avg length: 136 \t reward: -308.0650854007703\n",
            "Episode 600 \t avg length: 47 \t reward: -131.66532700372127\n",
            "Episode 620 \t avg length: 175 \t reward: -354.0902020538167\n",
            "Episode 640 \t avg length: 124 \t reward: -269.52366510478083\n",
            "Episode 660 \t avg length: 183 \t reward: -410.4831512798458\n",
            "Episode 680 \t avg length: 212 \t reward: -492.55142345422973\n",
            "Episode 700 \t avg length: 195 \t reward: -377.28323243531196\n",
            "Episode 720 \t avg length: 213 \t reward: -488.10134163551237\n",
            "Episode 740 \t avg length: 229 \t reward: -515.5762257713645\n",
            "Episode 760 \t avg length: 220 \t reward: -508.80878749756704\n",
            "Episode 780 \t avg length: 229 \t reward: -517.881327500632\n",
            "Episode 800 \t avg length: 225 \t reward: -490.95554996193016\n",
            "Episode 820 \t avg length: 223 \t reward: -493.1876736695979\n",
            "Episode 840 \t avg length: 188 \t reward: -413.22288700206366\n",
            "Episode 860 \t avg length: 214 \t reward: -491.51465823214147\n",
            "Episode 880 \t avg length: 214 \t reward: -497.2492072487602\n",
            "Episode 900 \t avg length: 215 \t reward: -494.1104226290369\n",
            "Episode 920 \t avg length: 231 \t reward: -484.16546664384276\n",
            "Episode 940 \t avg length: 274 \t reward: -533.2519626400393\n",
            "Episode 960 \t avg length: 208 \t reward: -456.42495649742324\n",
            "Episode 980 \t avg length: 224 \t reward: -494.1070842915553\n",
            "Episode 1000 \t avg length: 204 \t reward: -475.11139109174144\n",
            "Episode 1020 \t avg length: 200 \t reward: -468.1511645673769\n",
            "Episode 1040 \t avg length: 225 \t reward: -516.9843392836631\n",
            "Episode 1060 \t avg length: 198 \t reward: -467.12497736873286\n",
            "Episode 1080 \t avg length: 221 \t reward: -512.0738272280853\n",
            "Episode 1100 \t avg length: 225 \t reward: -516.2705704902689\n",
            "Episode 1120 \t avg length: 204 \t reward: -483.6747517183213\n",
            "Episode 1140 \t avg length: 198 \t reward: -468.8123738935127\n",
            "Episode 1160 \t avg length: 207 \t reward: -488.1971656197614\n",
            "Episode 1180 \t avg length: 188 \t reward: -448.3017300592893\n",
            "Episode 1200 \t avg length: 212 \t reward: -492.3870985144449\n",
            "Episode 1220 \t avg length: 210 \t reward: -489.7637410571795\n",
            "Episode 1240 \t avg length: 206 \t reward: -488.031846540247\n",
            "Episode 1260 \t avg length: 215 \t reward: -496.5550469218996\n",
            "Episode 1280 \t avg length: 201 \t reward: -473.7324678512734\n",
            "Episode 1300 \t avg length: 216 \t reward: -509.23561704609\n",
            "Episode 1320 \t avg length: 193 \t reward: -495.4612536450577\n",
            "Episode 1340 \t avg length: 201 \t reward: -497.52334417394786\n",
            "Episode 1360 \t avg length: 194 \t reward: -468.5432093143889\n",
            "Episode 1380 \t avg length: 198 \t reward: -472.1806320849917\n",
            "Episode 1400 \t avg length: 191 \t reward: -414.25177357769627\n",
            "Episode 1420 \t avg length: 20 \t reward: -109.9526994834742\n",
            "Episode 1440 \t avg length: 19 \t reward: -108.48832885802861\n",
            "Episode 1460 \t avg length: 15 \t reward: -106.90799196305515\n",
            "Episode 1480 \t avg length: 21 \t reward: -110.754446238898\n",
            "Episode 1500 \t avg length: 35 \t reward: -116.17660114460514\n",
            "Episode 1520 \t avg length: 71 \t reward: -173.68476377336518\n",
            "Episode 1540 \t avg length: 18 \t reward: -108.55730824089672\n",
            "Episode 1560 \t avg length: 99 \t reward: -217.7700165799904\n",
            "Episode 1580 \t avg length: 227 \t reward: -514.360580989944\n",
            "Episode 1600 \t avg length: 221 \t reward: -512.3103275975075\n",
            "Episode 1620 \t avg length: 211 \t reward: -493.4891423320124\n",
            "Episode 1640 \t avg length: 199 \t reward: -468.87245416427004\n",
            "Episode 1660 \t avg length: 214 \t reward: -497.2768847413443\n",
            "Episode 1680 \t avg length: 202 \t reward: -473.44953305072886\n",
            "Episode 1700 \t avg length: 221 \t reward: -513.6709021736272\n",
            "Episode 1720 \t avg length: 191 \t reward: -456.12146922798763\n",
            "Episode 1740 \t avg length: 228 \t reward: -519.7935873151513\n",
            "Episode 1760 \t avg length: 208 \t reward: -486.8909318980426\n",
            "Episode 1780 \t avg length: 209 \t reward: -490.0752533854735\n",
            "Episode 1800 \t avg length: 205 \t reward: -476.2160374005477\n",
            "Episode 1820 \t avg length: 202 \t reward: -471.3336756450782\n",
            "Episode 1840 \t avg length: 201 \t reward: -472.51867630346914\n",
            "Episode 1860 \t avg length: 199 \t reward: -470.33222709812355\n",
            "Episode 1880 \t avg length: 218 \t reward: -509.73073239823714\n",
            "Episode 1900 \t avg length: 209 \t reward: -476.15973736929936\n",
            "Episode 1920 \t avg length: 224 \t reward: -514.5776283224575\n",
            "Episode 1940 \t avg length: 216 \t reward: -493.14834868992284\n",
            "Episode 1960 \t avg length: 234 \t reward: -518.0680620252436\n",
            "Episode 1980 \t avg length: 221 \t reward: -491.5137553917285\n",
            "Episode 2000 \t avg length: 199 \t reward: -395.8078316603176\n",
            "Episode 2020 \t avg length: 21 \t reward: -110.04341575891287\n",
            "Episode 2040 \t avg length: 195 \t reward: -411.3398603343958\n",
            "Episode 2060 \t avg length: 202 \t reward: -473.86672304156417\n",
            "Episode 2080 \t avg length: 212 \t reward: -489.9459936475629\n",
            "Episode 2100 \t avg length: 221 \t reward: -513.484629159511\n",
            "Episode 2120 \t avg length: 216 \t reward: -499.10073623864275\n",
            "Episode 2140 \t avg length: 218 \t reward: -506.8887922161436\n",
            "Episode 2160 \t avg length: 175 \t reward: -424.41497745517944\n",
            "Episode 2180 \t avg length: 214 \t reward: -497.25409977051925\n",
            "Episode 2200 \t avg length: 222 \t reward: -516.0219974357619\n",
            "Episode 2220 \t avg length: 216 \t reward: -497.32741843035336\n",
            "Episode 2240 \t avg length: 211 \t reward: -491.8788301844694\n",
            "Episode 2260 \t avg length: 209 \t reward: -493.68430474190734\n",
            "Episode 2280 \t avg length: 197 \t reward: -467.24028464065566\n",
            "Episode 2300 \t avg length: 198 \t reward: -469.6309561703283\n",
            "Episode 2320 \t avg length: 211 \t reward: -492.58712928972574\n",
            "Episode 2340 \t avg length: 210 \t reward: -491.5366951827047\n",
            "Episode 2360 \t avg length: 207 \t reward: -488.7714511552366\n",
            "Episode 2380 \t avg length: 183 \t reward: -436.119615655232\n",
            "Episode 2400 \t avg length: 227 \t reward: -519.7518455344277\n",
            "Episode 2420 \t avg length: 209 \t reward: -486.12587073723125\n",
            "Episode 2440 \t avg length: 222 \t reward: -510.94195196002113\n",
            "Episode 2460 \t avg length: 213 \t reward: -492.6389683030795\n",
            "Episode 2480 \t avg length: 219 \t reward: -508.72052372840216\n",
            "Episode 2500 \t avg length: 199 \t reward: -470.17483114169283\n",
            "Episode 2520 \t avg length: 201 \t reward: -473.78532920480745\n",
            "Episode 2540 \t avg length: 198 \t reward: -471.5849688711749\n",
            "Episode 2560 \t avg length: 218 \t reward: -507.6132373223795\n",
            "Episode 2580 \t avg length: 222 \t reward: -511.56513127067785\n",
            "Episode 2600 \t avg length: 197 \t reward: -467.7491109410841\n",
            "Episode 2620 \t avg length: 189 \t reward: -452.11565403733766\n",
            "Episode 2640 \t avg length: 223 \t reward: -514.811757001077\n",
            "Episode 2660 \t avg length: 223 \t reward: -511.7484564108414\n",
            "Episode 2680 \t avg length: 219 \t reward: -511.94894048521735\n",
            "Episode 2700 \t avg length: 216 \t reward: -507.4563255376398\n",
            "Episode 2720 \t avg length: 222 \t reward: -510.767465431024\n",
            "Episode 2740 \t avg length: 201 \t reward: -472.0576773745015\n",
            "Episode 2760 \t avg length: 210 \t reward: -489.1044447880263\n",
            "Episode 2780 \t avg length: 212 \t reward: -491.7990729206193\n",
            "Episode 2800 \t avg length: 217 \t reward: -506.40127429585755\n",
            "Episode 2820 \t avg length: 226 \t reward: -515.3321582650983\n",
            "Episode 2840 \t avg length: 222 \t reward: -515.6499078287669\n",
            "Episode 2860 \t avg length: 200 \t reward: -471.28983821289614\n",
            "Episode 2880 \t avg length: 221 \t reward: -513.1623821798228\n",
            "Episode 2900 \t avg length: 226 \t reward: -517.1462543671007\n",
            "Episode 2920 \t avg length: 207 \t reward: -485.61150990942815\n",
            "Episode 2940 \t avg length: 225 \t reward: -517.6052227190396\n",
            "Episode 2960 \t avg length: 219 \t reward: -507.2998491262244\n",
            "Episode 2980 \t avg length: 222 \t reward: -515.1660127602565\n",
            "Episode 3000 \t avg length: 195 \t reward: -466.01666364467326\n",
            "Episode 3020 \t avg length: 212 \t reward: -495.65095419205244\n",
            "Episode 3040 \t avg length: 224 \t reward: -517.9853377101042\n",
            "Episode 3060 \t avg length: 209 \t reward: -488.7989436138567\n",
            "Episode 3080 \t avg length: 180 \t reward: -431.16130757186045\n",
            "Episode 3100 \t avg length: 221 \t reward: -509.6972147921094\n",
            "Episode 3120 \t avg length: 223 \t reward: -515.6315767272139\n",
            "Episode 3140 \t avg length: 221 \t reward: -511.7792485242788\n",
            "Episode 3160 \t avg length: 219 \t reward: -509.2749397594483\n",
            "Episode 3180 \t avg length: 190 \t reward: -454.32293296816\n",
            "Episode 3200 \t avg length: 212 \t reward: -491.3571278078627\n",
            "Episode 3220 \t avg length: 211 \t reward: -489.2187060244002\n",
            "Episode 3240 \t avg length: 210 \t reward: -492.85344875511566\n",
            "Episode 3260 \t avg length: 204 \t reward: -484.22611079238385\n",
            "Episode 3280 \t avg length: 209 \t reward: -490.93630868489646\n",
            "Episode 3300 \t avg length: 215 \t reward: -504.5213389578028\n",
            "Episode 3320 \t avg length: 222 \t reward: -513.4051090620653\n",
            "Episode 3340 \t avg length: 224 \t reward: -516.4360491772215\n",
            "Episode 3360 \t avg length: 212 \t reward: -494.147251011821\n",
            "Episode 3380 \t avg length: 210 \t reward: -490.3546422540788\n",
            "Episode 3400 \t avg length: 203 \t reward: -475.04886544738486\n",
            "Episode 3420 \t avg length: 229 \t reward: -518.9492990749407\n",
            "Episode 3440 \t avg length: 221 \t reward: -510.24632880949014\n",
            "Episode 3460 \t avg length: 215 \t reward: -498.99074655786916\n",
            "Episode 3480 \t avg length: 208 \t reward: -491.4459734099777\n",
            "Episode 3500 \t avg length: 213 \t reward: -491.7348724355237\n",
            "Episode 3520 \t avg length: 213 \t reward: -494.1910785479205\n",
            "Episode 3540 \t avg length: 210 \t reward: -492.9602447889859\n",
            "Episode 3560 \t avg length: 201 \t reward: -469.4949217911024\n",
            "Episode 3580 \t avg length: 224 \t reward: -516.6570380770597\n",
            "Episode 3600 \t avg length: 202 \t reward: -475.85601079318286\n",
            "Episode 3620 \t avg length: 216 \t reward: -507.0216410971552\n",
            "Episode 3640 \t avg length: 207 \t reward: -478.11148224142937\n",
            "Episode 3660 \t avg length: 217 \t reward: -507.3395567784688\n",
            "Episode 3680 \t avg length: 209 \t reward: -490.6891910661485\n",
            "Episode 3700 \t avg length: 215 \t reward: -493.32501400018384\n",
            "Episode 3720 \t avg length: 214 \t reward: -495.30079554250176\n",
            "Episode 3740 \t avg length: 211 \t reward: -490.89866160548127\n",
            "Episode 3760 \t avg length: 192 \t reward: -462.9233943008802\n",
            "Episode 3780 \t avg length: 211 \t reward: -503.9415579903863\n",
            "Episode 3800 \t avg length: 212 \t reward: -494.38559571960093\n",
            "Episode 3820 \t avg length: 214 \t reward: -502.6399386283937\n",
            "Episode 3840 \t avg length: 213 \t reward: -492.0140921196538\n",
            "Episode 3860 \t avg length: 222 \t reward: -515.7947465043732\n",
            "Episode 3880 \t avg length: 225 \t reward: -517.6395029943981\n",
            "Episode 3900 \t avg length: 219 \t reward: -509.7943004277005\n",
            "Episode 3920 \t avg length: 223 \t reward: -515.4679399835967\n",
            "Episode 3940 \t avg length: 222 \t reward: -514.6527758158109\n",
            "Episode 3960 \t avg length: 209 \t reward: -490.14953202204396\n",
            "Episode 3980 \t avg length: 207 \t reward: -489.46527017585214\n",
            "Episode 4000 \t avg length: 219 \t reward: -510.4477686517087\n",
            "Episode 4020 \t avg length: 224 \t reward: -516.4411254181799\n",
            "Episode 4040 \t avg length: 215 \t reward: -496.02310486382595\n",
            "Episode 4060 \t avg length: 207 \t reward: -487.8698868092084\n",
            "Episode 4080 \t avg length: 208 \t reward: -479.5754703590657\n",
            "Episode 4100 \t avg length: 217 \t reward: -497.0092343963298\n",
            "Episode 4120 \t avg length: 209 \t reward: -490.8771404720325\n",
            "Episode 4140 \t avg length: 191 \t reward: -453.352014046434\n",
            "Episode 4160 \t avg length: 201 \t reward: -476.0459234984414\n",
            "Episode 4180 \t avg length: 201 \t reward: -471.7542228064173\n",
            "Episode 4200 \t avg length: 220 \t reward: -512.5313546097373\n",
            "Episode 4220 \t avg length: 222 \t reward: -516.0074349385716\n",
            "Episode 4240 \t avg length: 211 \t reward: -493.4929572493282\n",
            "Episode 4260 \t avg length: 226 \t reward: -517.2308130823956\n",
            "Episode 4280 \t avg length: 223 \t reward: -512.8445327211158\n",
            "Episode 4300 \t avg length: 200 \t reward: -472.7667532029689\n",
            "Episode 4320 \t avg length: 210 \t reward: -489.6938559133433\n",
            "Episode 4340 \t avg length: 210 \t reward: -490.42707121990077\n",
            "Episode 4360 \t avg length: 222 \t reward: -516.1295491470762\n",
            "Episode 4380 \t avg length: 209 \t reward: -478.4033671995895\n",
            "Episode 4400 \t avg length: 208 \t reward: -490.1869731118824\n",
            "Episode 4420 \t avg length: 201 \t reward: -475.8518586546058\n",
            "Episode 4440 \t avg length: 225 \t reward: -517.9981571100836\n",
            "Episode 4460 \t avg length: 199 \t reward: -468.8742312632242\n",
            "Episode 4480 \t avg length: 224 \t reward: -514.725100917359\n",
            "Episode 4500 \t avg length: 227 \t reward: -516.1015506358301\n",
            "Episode 4520 \t avg length: 221 \t reward: -516.3209757617424\n",
            "Episode 4540 \t avg length: 208 \t reward: -485.8613869421894\n",
            "Episode 4560 \t avg length: 212 \t reward: -491.8037605286436\n",
            "Episode 4580 \t avg length: 219 \t reward: -508.3899845434782\n",
            "Episode 4600 \t avg length: 216 \t reward: -507.53658727696467\n",
            "Episode 4620 \t avg length: 217 \t reward: -498.6028495537636\n",
            "Episode 4640 \t avg length: 203 \t reward: -477.1711590665054\n",
            "Episode 4660 \t avg length: 214 \t reward: -491.21181293584687\n",
            "Episode 4680 \t avg length: 205 \t reward: -486.8414614723987\n",
            "Episode 4700 \t avg length: 201 \t reward: -472.6904674756047\n",
            "Episode 4720 \t avg length: 214 \t reward: -499.6546707712182\n",
            "Episode 4740 \t avg length: 226 \t reward: -517.2235283536336\n",
            "Episode 4760 \t avg length: 225 \t reward: -517.4830741723\n",
            "Episode 4780 \t avg length: 224 \t reward: -516.5297654647254\n",
            "Episode 4800 \t avg length: 209 \t reward: -488.1446833349767\n",
            "Episode 4820 \t avg length: 221 \t reward: -511.2918398835416\n",
            "Episode 4840 \t avg length: 221 \t reward: -512.3489053736887\n",
            "Episode 4860 \t avg length: 202 \t reward: -475.3257700990648\n",
            "Episode 4880 \t avg length: 207 \t reward: -483.808532719651\n",
            "Episode 4900 \t avg length: 209 \t reward: -487.51025340296053\n",
            "Episode 4920 \t avg length: 217 \t reward: -507.1924009740981\n",
            "Episode 4940 \t avg length: 225 \t reward: -514.4451441387511\n",
            "Episode 4960 \t avg length: 222 \t reward: -513.0641196788042\n",
            "Episode 4980 \t avg length: 223 \t reward: -515.7264620173905\n",
            "Episode 5000 \t avg length: 220 \t reward: -511.69905702915764\n",
            "Episode 5020 \t avg length: 219 \t reward: -508.8703329052216\n",
            "Episode 5040 \t avg length: 214 \t reward: -495.5387197058041\n",
            "Episode 5060 \t avg length: 225 \t reward: -517.1569588821327\n",
            "Episode 5080 \t avg length: 209 \t reward: -491.9885864423516\n",
            "Episode 5100 \t avg length: 199 \t reward: -471.22844684579525\n",
            "Episode 5120 \t avg length: 221 \t reward: -510.06456140713607\n",
            "Episode 5140 \t avg length: 209 \t reward: -491.4478032631843\n",
            "Episode 5160 \t avg length: 224 \t reward: -515.8018966556767\n",
            "Episode 5180 \t avg length: 210 \t reward: -490.9831763374306\n",
            "Episode 5200 \t avg length: 221 \t reward: -511.6493482306555\n",
            "Episode 5220 \t avg length: 222 \t reward: -514.2005582541703\n",
            "Episode 5240 \t avg length: 206 \t reward: -486.2157106573524\n",
            "Episode 5260 \t avg length: 220 \t reward: -511.38562765020015\n",
            "Episode 5280 \t avg length: 214 \t reward: -498.89072212638985\n",
            "Episode 5300 \t avg length: 199 \t reward: -473.30580674773853\n",
            "Episode 5320 \t avg length: 223 \t reward: -514.6556841974506\n",
            "Episode 5340 \t avg length: 197 \t reward: -467.814041752196\n",
            "Episode 5360 \t avg length: 218 \t reward: -510.6638852618623\n",
            "Episode 5380 \t avg length: 209 \t reward: -491.9178523136869\n",
            "Episode 5400 \t avg length: 209 \t reward: -491.05900283388553\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3bca163389d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# train from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0micfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInitialCondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mhistory\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_condition_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0micfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# load from pth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-6a98a3126fe3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, initial_condition, initial_condition_fn, episodes)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Running policy_old:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_old\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;31m# Saving reward and is_terminal:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a41f55c66926>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mt_bound_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshoot_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shoot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_bound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mt_bound_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a41f55c66926>\u001b[0m in \u001b[0;36m_shoot\u001b[0;34m(self, A1, A2, f1, f2, vec_initial, t_start, t_bound, method)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mee\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mee\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'terminal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         solver = scipy.integrate.solve_ivp(fun, (t_start, t_bound), vec_initial, method=method, events=events,\n\u001b[0m\u001b[1;32m     81\u001b[0m                                             rtol=1e-4, atol=1e-7, max_step=.03, first_step=.001, dense_output=True)\n\u001b[1;32m     82\u001b[0m         \u001b[0mu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mA1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mCt\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mCd\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Leader velocity (constant)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/integrate/_ivp/ivp.py\u001b[0m in \u001b[0;36msolve_ivp\u001b[0;34m(fun, t_span, y0, method, t_eval, dense_output, events, vectorized, args, **options)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'finished'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/integrate/_ivp/base.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/integrate/_ivp/rk.py\u001b[0m in \u001b[0;36m_step_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mh_abs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             y_new, f_new = rk_step(self.fun, t, y, self.f, h, self.A,\n\u001b[0m\u001b[1;32m    145\u001b[0m                                    self.B, self.C, self.K)\n\u001b[1;32m    146\u001b[0m             \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/integrate/_ivp/rk.py\u001b[0m in \u001b[0;36mrk_step\u001b[0;34m(fun, t, y, f, h, A, B, C, K)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.test_initial_conditions(InitialCondition(distance=dd, f2=1.2) for dd in range(20, 65, 10))"
      ],
      "metadata": {
        "id": "h8dSJB6F76DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the policy\n",
        "# fa_range = numpy.linspace(-10, 10, 100)\n",
        "# f2_range = numpy.linspace(0.5, 1.5, 100)\n",
        "# a2_range = numpy.linspace(0, 3, 100)\n",
        "# d_range = numpy.linspace(0, 50, 100)\n",
        "# u_range = numpy.linspace(0, 30, 100)\n",
        "# p_range = numpy.linspace(0, 3, 100)\n",
        "# trainer.plot_policy(\n",
        "#     fa_range=fa_range,\n",
        "#     f2_range=f2_range,\n",
        "#     a2_range=a2_range,\n",
        "#     d_range=d_range,\n",
        "#     u_range=u_range,\n",
        "#     p_range=p_range\n",
        "# )"
      ],
      "metadata": {
        "id": "ksMYd2CQ5zeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.create_video(time=20, ic=InitialCondition(distance=30, A2=2.5))"
      ],
      "metadata": {
        "id": "x4_YXH-xnyX9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}